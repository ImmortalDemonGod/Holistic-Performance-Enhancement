https://dev.to/teamcamp/daily-logs-for-devs-how-a-5-minute-habit-can-10x-your-weekly-output-4peb

https://www.nature.com/articles/s41586-025-09041-8

https://iopscience.iop.org/article/10.1088/1361-6633/adc82e/pdf

https://pmc.ncbi.nlm.nih.gov/articles/PMC11235642/

https://phys.org/news/2025-06-chimera-approach-mitochondrial-barrier-protein.html#amp_tf=From%20%251%24s&aoh=17496582471982&csi=0&referrer=https%3A%2F%2Fwww.google.com&ampshare=https%3A%2F%2Fphys.org%2Fnews%2F2025-06-chimera-approach-mitochondrial-barrier-protein.html

https://arxiv.org/html/2503.11842v1

https://dev.to/teamcamp/the-4-hour-focus-block-advanced-deep-work-techniques-for-senior-developers-4136

https://www.nature.com/articles/s41589-025-01922-3

https://arxiv.org/pdf/2505.24832

https://arxiv.org/abs/2412.06769 | [2412.06769] Training Large Language Models to Reason in a Continuous Latent Space
https://arxiv.org/abs/2407.10188 | [2407.10188] Unexpected Benefits of Self-Modeling in Neural Systems
https://arxiv.org/abs/2412.13663 | [2412.13663] Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference
https://arxiv.org/abs/2104.13478 | [2104.13478] Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges
https://arxiv.org/abs/2206.14486 | [2206.14486] Beyond neural scaling laws: beating power law scaling via data pruning
https://arxiv.org/abs/2110.06804 | [2110.06804] A comprehensive review of Binary Neural Network

https://arxiv.org/abs/2412.04332 | [2412.04332] Liquid: Language Models are Scalable and Unified Multi-modal Generators
https://arxiv.org/abs/2005.10636 | [2005.10636] Graph-based, Self-Supervised Program Repair from Diagnostic Feedback
https://www.cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization | The Practitionerâ€™s Guide to the Maximal Update Parameterization - Cerebras
https://arxiv.org/abs/2412.06975 | [2412.06975] AutoReason: Automatic Few-Shot Reasoning Decomposition
https://arxiv.org/abs/2305.14314 | [2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs
https://arxiv.org/abs/2501.00070 | [2501.00070] ICLR: In-Context Learning of Representations
https://arxiv.org/abs/2212.04089 | [2212.04089] Editing Models with Task Arithmetic

https://www.nature.com/articles/s41586-024-08328-6 | Accurate predictions on small data with a tabular foundation model | Nature
https://arxiv.org/html/2412.12153v1#:~:text=A%20simple%20way%20to%20merge,33%20%2C%20%2040%2C%2010 | Revisiting Weight Averaging for Model Merging
https://www.researchgate.net/publication/349839055_An_Analytical_Study_of_Code_Smells | (PDF) An Analytical Study of Code Smells
https://arxiv.org/abs/2103.01088 | [2103.01088] Code smells: A Synthetic Narrative Review
https://arxiv.org/abs/2312.17217 | [2312.17217] Close encounters of the primordial kind: a new observable for primordial black holes as dark matter
https://arxiv.org/abs/2011.14867 | [2011.14867] A Survey on Heterogeneous Graph Embedding: Methods, Techniques, Applications and Sources

https://arxiv.org/abs/2405.20233 | [2405.20233] Grokfast: Accelerated Grokking by Amplifying Slow Gradients
https://arxiv.org/abs/2403.00043?utm_source=chatgpt.com | [2403.00043] RiNALMo: General-Purpose RNA Language Models Can Generalize Well on Structure Prediction Tasks
https://www.biorxiv.org/content/10.1101/2024.11.28.625345v1?utm_source=chatgpt.com | A Large-Scale Foundation Model for RNA Function and Structure Prediction | bioRxiv
https://arxiv.org/html/2411.17525v1 | Pushing the Limits of Large Language Model Quantization via the Linearity Theorem
https://arxiv.org/abs/2505.12540 | [2505.12540] Harnessing the Universal Geometry of Embeddings
https://arxiv.org/abs/1911.01547 | [1911.01547] On the Measure of Intelligence

https://arxiv.org/abs/2412.04604 | [2412.04604] ARC Prize 2024: Technical Report
https://arxiv.org/abs/2505.11831 | [2505.11831] ARC-AGI-2: A New Challenge for Frontier AI Reasoning Systems
https://arxiv.org/abs/2504.12285 | [2504.12285] BitNet b1.58 2B4T Technical Report
https://arxiv.org/abs/2409.10502 | [2409.10502] Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles

https://arxiv.org/pdf/1608.05343 | Decoupled Neural Interfaces using Synthetic Gradients
https://arxiv.org/abs/2410.23168 | [2410.23168] TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters
https://arxiv.org/abs/2411.16085 | [2411.16085] Cautious Optimizers: Improving Training with One Line of Code
https://arxiv.org/abs/2305.01582 | [2305.01582] Interpretable Machine Learning for Science with PySR and SymbolicRegression.jl
https://arxiv.org/abs/2412.08821 | [2412.08821] Large Concept Models: Language Modeling in a Sentence Representation Space
https://ai.meta.com/research/publications/byte-latent-transformer-patches-scale-better-than-tokens/ | Byte Latent Transformer: Patches Scale Better Than Tokens | Research - AI at Meta

https://arxiv.org/abs/2210.09880 | [2210.09880] Graphs, Constraints, and Search for the Abstraction and Reasoning Corpus
https://arxiv.org/abs/2411.10438 | [2411.10438] MARS: Unleashing the Power of Variance Reduction for Training Large Models
https://arxiv.org/abs/2212.04089 | [2212.04089] Editing Models with Task Arithmetic
https://arxiv.org/html/2406.02847v2 | Exact Conversion of In-Context Learning to Model Weights in Linearized-Attention Transformers
https://arxiv.org/abs/2505.24832 | [2505.24832] How much do language models memorize?
https://arxiv.org/abs/2303.18184 | [2303.18184] A Survey on Automated Program Repair Techniques
https://arxiv.org/abs/1706.03762 | [1706.03762] Attention Is All You Need
https://transformer-circuits.pub/2025/attribution-graphs/biology.html | On the Biology of a Large Language Model
https://www.anthropic.com/research/mapping-mind-language-model | Mapping the Mind of a Large Language Model \ Anthropic
https://www.anthropic.com/news/tracing-thoughts-language-model | Tracing the thoughts of a large language model \ Anthropic
