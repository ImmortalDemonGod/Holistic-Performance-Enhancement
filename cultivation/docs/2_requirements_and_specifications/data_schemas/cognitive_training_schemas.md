# Cognitive Augmentation Domain: Data Schemas

**Document ID:** `COG-AUG-SCHEMAS-V1.0`
**Version:** 1.0
**Date Created:** 2025-06-11
**Status:** Definitive Proposal
**Parent Blueprint:** [`mentat_os_blueprint.md`](../../5_domain_knowledge_and_curricula/cognitive_augmentation/mentat_os_blueprint.md)

## 1. Overview

This document specifies the data schemas for the **Mentat-OS Cognitive Augmentation Domain**. It defines the structure and data types for the raw log files generated during cognitive drills and the aggregated weekly Parquet files used by the Holistic Integration Layer (HIL). Adherence to these schemas is mandatory for all related scripts to ensure data integrity and pipeline reliability.

---

## 2. Raw Data Schema: `cognitive_drill_log.csv`

This schema defines the output format for the `mentat_autograder.py` script. Each row represents a single completed drill session. The data is appended to a CSV file for simplicity and robustness.

**File Location:** `cultivation/data/cognitive_augmentation/raw/cognitive_drill_log.csv`

| Column Name | Data Type | Example | Description & Constraints |
| :--- | :--- | :--- | :--- |
| **timestamp_utc** | String (ISO 8601) | `"2025-06-11T14:05:12.345Z"` | The UTC timestamp when the drill session was completed and logged. **Required.** |
| **user_id** | String | `"MIGUEL"` | The identifier for the user performing the drill. **Required.** |
| **session_id** | String | `"2025-06-11-AM"` | A user-defined identifier for the overall training block or session (e.g., a daily session). **Required.** |
| **drill_id** | String | `"D3"` | The unique identifier for the drill, corresponding to an entry in the `drill_cookbook.md`. **Required.** |
| **kpi_code** | String | `"WM-Span"` | The machine-readable Key Performance Indicator code associated with the drill. **Required.** |
| **score** | Float / Integer | `22.0` | The raw, quantitative score achieved in the drill. The unit is defined by the KPI. **Required.** |
| **pass_flag** | Boolean | `True` | A boolean flag indicating whether the score met the success threshold defined for the KPI. **Required.** |
| **notes** | String | `"Felt focused, digits felt clear."` | Optional, user-provided qualitative notes or reflections on the drill performance. Can be null/empty. |

---

## 3. Processed Data Schema: `cognitive_training_weekly.parquet`

This schema defines the structure for the aggregated weekly data file, which is the primary input for the Potential Engine (Π) and other HIL components. It is generated by the `process_cognitive_drills.py` ETL script.

**File Location:** `cultivation/data/cognitive_augmentation/processed/cognitive_training_weekly.parquet`

| Column Name | Data Type | Example | Description & Constraints |
| :--- | :--- | :--- | :--- |
| **week_iso** | String | `"2025-W24"` | The ISO week identifier (YYYY-Www) for the aggregation period. **Required, PK.** |
| **total_drill_sessions**| Integer | `35` | The total count of all drill sessions logged during that week. |
| **total_drill_time_min**| Float | `75.5` | The total time in minutes spent on all cognitive drills during the week. |
| **kpi_wm_span_avg** | Float | `21.5` | The average score for the `WM-Span` KPI across all `D3` drill sessions this week. |
| **kpi_wm_span_max** | Integer | `23` | The maximum score achieved for the `WM-Span` KPI this week. |
| **kpi_logic_acc_avg** | Float (0.0-1.0)| `0.92` | The average accuracy score for logic-based drills. (Placeholder for when a logic drill is added). |
| **kpi_math_sps_avg** | Float | `52.5` | The average score (ops/min) for the `Math-SPS` KPI across all `D5` drill sessions this week. |
| **kpi_parity_ec_rate_avg** | Float (0.0-1.0)| `0.85` | The average error catch rate from D4/parity drills. |
| **kpi_parity_ec_avg** | Float (0.0-1.0)| `0.88` | The average error-catch rate for the `Parity-EC` KPI across all parity drills this week. (General or to be reviewed) |
| **kpi_fermi_rmse_avg** | Float | `1.85` | The average root-mean-square error ratio for the `Fermi-RMSE` KPI across all `D6` drills this week. |
| **kpi_analogy_quality_avg** | Float (1.0-5.0)| `4.2` | The average self-rated score for the `Analogy-Quality-Score` KPI across all `D2` drills this week. |
| **kpi_interoceptive_label_accuracy_avg** | Float (0.0-1.0) | `0.75` | Average accuracy in labeling interoceptive states from D6 drills. (Simpler MVP metric) |
| **kpi_somatic_insight_rate_avg**| Float (0.0-1.0)| `0.67` | The average success rate for the `Somatic-Insight-Rate` KPI across all `D6` drills this week. (Advanced KPI, potentially downstream synergy metric) |

**Notes on the Processed Schema:**
*   This is an initial schema. New `kpi_*_avg` columns will be added as new measurable drills are introduced.
*   The `process_cognitive_drills.py` ETL script is responsible for reading `cognitive_drill_log.csv`, grouping by week and `kpi_code`, calculating the weekly averages, and writing to this Parquet file.
*   The schema should be enforced using a validation library like Pandera within the ETL script's CI tests.

---

## 4. Rationale for Schema Design

*   **Raw Data (`.csv`):**
    *   **Append-Only:** Using a simple CSV for raw logs allows for easy, robust, append-only operations. It is human-readable and less prone to corruption than a binary format for simple, frequent writes.
    *   **Self-Contained Rows:** Each row contains all necessary context (`user_id`, `session_id`, `drill_id`), making it independent and easy to process without complex joins during the initial ETL phase.

*   **Processed Data (`.parquet`):**
    *   **Columnar Format:** Parquet is highly efficient for analytical queries. The Potential Engine (Π) will only need to read a few columns from the latest week, and Parquet is optimized for this access pattern.
    *   **Typed & Compressed:** Parquet enforces data types and provides excellent compression, making it ideal for storing structured numerical data for analysis.
    *   **Weekly Aggregation:** Aggregating data weekly aligns with the cadence of the Synergy and Potential engines and provides a stable, predictable input for those systems.

This two-tiered data strategy separates the rapid, transactional logging of individual drills from the structured, analytical needs of the downstream holistic models, providing a robust and scalable data foundation for the Mentat-OS domain.
