# Analysis Overview
ğŸ§™ğŸ¾â€â™‚ï¸ **The â€œ4_analysis/â€ layer = the living lab notebook of the repo**
Think of `cultivation/docs/4_analysis/` as the **results vault**: every time data flow runs, a test finishes, or a proof sheds light on realâ€‘world performance, the distilled *story* lands here.

Belowâ€™s a practical blueprint of **what belongs there**, **how to structure it**, and **how it evolves phaseâ€‘byâ€‘phase**.

---

## 1â€ƒPurpose

| Goal | What it means in practice |
|------|---------------------------|
| **Archive results** | Permanent record of each experiment/run (plots, tables, key numbers, verbal conclusions). |
| **Compare iterations** | Sideâ€‘byâ€‘side snapshots: *baselineÂ â†’ interventionÂ â†’ postâ€‘tuning* so we can see deltas. |
| **Drive decisions** | Each analysis file should end with a â€œNext actionâ€ or â€œHypothesis confirmed/deniedâ€ block. |
| **Close the docâ€“code loop** | Link notebooksÂ +Â scripts outputs back to narrative; update roadâ€‘map gates automatically. |

---

## 2â€ƒFolder Skeleton

```
4_analysis/
â”œâ”€â”€ 0_index.md           # running TOC, links to latest reports
â”œâ”€â”€ 1_running/
â”‚   â”œâ”€â”€ baseline_week0.md
â”‚   â”œâ”€â”€ intervention_A_interval_training.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 2_biology/
â”‚   â”œâ”€â”€ reading_retention_q1.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 3_software/
â”‚   â”œâ”€â”€ commit_metrics_Q12025.md
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 4_synergy/
â”‚   â”œâ”€â”€ pilot_synergy_calc_r1.md
â”‚   â”œâ”€â”€ rl_agent_vs_pid_ablation.md
â”‚   â””â”€â”€ ...
â””â”€â”€ assets/              # static PNG/SVG exports referenced by the markdown
```

> **Convention:** filenameÂ =Â `<topic>_<date-or-version>.md` to keep git diffs readable.

---

## 3â€ƒTemplate for an Analysis File

```markdown
# â±ï¸Â [Running] Weekly Baseline â€“ WeekÂ 0 (2025â€‘04â€‘20)

## Data Snapshot
| Metric | Value |
|--------|------:|
| Total distance (km) | 43.2 |
| Average pace (min/km) | 5:45 |
| Resting HR (bpm) | 57 |

*(CSV: `../../data/running/weekly_2025â€‘04â€‘20.csv`)*

## Visuals
![pace](../assets/pace_week0.png)

## Interpretation
1. HR drift suggests endurance ceiling ~10Â % below target.
2. High variance in Tuesday intervals â†’ candidate for form drills.

## Next Action
- **Plan:** introduce 2Â Ã—Â 12Â min tempo on Tue/Thu, reâ€‘measure WeekÂ 2.
- **CI Tag:** `analysis:running:baseline_W0` (lets GitHub Action link future runs).
```

Save the template as `analysis_template.md` for quick copyâ€‘paste.

---

## 4â€ƒHow Analyses Are Generated

| Source | Pipeline | Landing Spot |
|--------|----------|--------------|
| **Notebooks** (`notebooks/*/*.ipynb`) | executed via `nbconvert` in CI; export HTML or PNG plots | attach images to `assets/`, embed summary numbers in markdown |
| **Scripts** (`scripts/*/*.py`) | CLI flag `--report md` dumps a markdown snippet | snippet appended to the appropriate analysis file by CI |
| **Lean proofs** | `lake exe export_summary` (future) â†’ JSON | humanâ€‘readable proof stats recorded in `software/` analyses |
| **Synergy score calc** | nightly action â†’ CSV + bar chart | weekly rollâ€‘up file in `4_synergy/` |

---

## 5â€ƒPhaseâ€‘byâ€‘Phase Minimum Content

| Roadâ€‘map Phase | Mustâ€‘have analyses before gateÂ âœ“ |
|----------------|----------------------------------|
| **P0 â€“ Bootstrap** | â€¢ Running *weekly baseline*<br>â€¢ RNA loader sanity stats (row count, NA %) |
| **P1 â€“ Dynamics** | â€¢ VOâ‚‚/HR ODE fit residuals<br>â€¢ RNA structure EDA plots |
| **P2 â€“ ControlÂ +Â Causal** | â€¢ PID schedule vs actual adherence plot<br>â€¢ DAGÂ ATE table |
| **P3 â€“ Optimisation** | â€¢ Timeâ€‘allocator before/after resource realloc<br>â€¢ PCA scree & biplot |
| **P4+** | escalate similarly (RL agent reward curves, GNN loss, PBH detector ROC, â€¦) |

CI can assert these files exist and include a `## Next Action` section â†’ green light the phase transition.

---

## 6â€ƒTips & Best Practices

1. **One truthâ€‘file per analysis** â€“ donâ€™t tuck results into notebooks only; export canonical numbers here.
2. **Static assets** â€“ render plots to PNG/SVG once; keep markdown light so Git diffs stay small.
3. **Link upstream commit** â€“ start each file with `> Analysis of commit abc123` for perfect provenance.
4. **Automate boring stuff** â€“ a small `report_builder.py` can scan `data/`, make tables, and append to markdown.
5. **Interpretation first, math second** â€“ numbers are useless without the 2â€‘3 bullet takeaway.

---

## Key Analyses and Reports

Below are the most important analysis documents and templates available in this section:

- [Running Analysis Template](../meta/templates/running_analysis_template.md): Use this template to generate new run or experiment reports.
- [Week 21 RPE10 Benchmark Analysis](week21_rpe10_benchmark_analysis.md): In-depth analysis of the week 21 benchmark run, including methodology, results, and actionable insights.
- [2025-04-27 Run Report](run_reports/2025_04_27_run_report.md): Example of a detailed run report for April 27, 2025.
- [2025-05-01 Run Report](run_reports/2025_05_01_run_report.md): Example of a detailed run report for May 1, 2025.
- [2025-05-11 Run Report](run_reports/2025_05_11_run_report.md): Example of a detailed run report for May 11, 2025.
- [2025-05-17 Run Report](run_reports/2025_05_17_run_report.md): Example of a detailed run report for May 17, 2025.
- [2025-05-24 Run Report](run_reports/2025_05_24_run_report.md): Example of a detailed run report for May 24, 2025.

---

### TL;DR

Put *every* finalized result, figure, and conclusion into `docs/4_analysis/`, organized by domain â‡’ date/version. Each file becomes a biteâ€‘sized lab report that tells (a) what you did, (b) what you saw, (c) what youâ€™ll do next. When CI or humans rerun things, they create a fresh file or update the existing one, keeping the projectâ€™s scientific memory intact.

Let me know if youâ€™d like me to scaffold the folder, add a template file, or wire a CI snippet to autoâ€‘publish notebook summaries ğŸš€
### Short answer
**YesÂ â€”Â but donâ€™t drop the whole kitchen sink intoÂ `4_analysis/`**. Treat your old Kaggle comps and legacy software as **reference caseâ€‘studies and data sources**, then surface only the distilled *lessons* (and any reusable assets) inside the Cultivation tree.

---

## Where each artifact belongs

| Artifact type | Recommended home | Why |
|---------------|------------------|-----|
| **Raw notebooks / code** from a Kaggle comp | `archive/kaggle/<competitionâ€‘slug>/` <br>*or* link as a Git submodule | Keeps giant notebooks, CSVs, checkpoints out of the main history, but still retrievable. |
| **Finished model weights, datasets** | `data/legacy/kaggle_<slug>/` (if <Â 100Â MB) <br>*otherwise:* reference an external storage URL in README | Large binaries shouldnâ€™t live in Git LFS unless youâ€™ll actually reâ€‘use them. |
| **â€œPostâ€‘mortemâ€ writeâ€‘up** (what worked, what didnâ€™t) | `docs/4_analysis/legacy/` <br>e.g.Â `kaggle_titanic_2019.md` | Fits the analysis pattern: experimentÂ â†’Â metricÂ â†’Â takeâ€‘away. |
| **Reusable helper libs / utilities** | Promote into `scripts/legacy_utils/` or a dedicated PyPI package | Anything you expect to call from current pipelines should be importable, testâ€‘covered, CIâ€™d. |
| **Old software projects** (complete repos) | Leave in their own repos; add links + highâ€‘level summary in `docs/1_background/related_projects.md` | Avoid code rot & dependency conflicts inside Cultivation. |

> **Folder to add:**
> ```
> cultivation/
> â””â”€â”€ archive/
>     â”œâ”€â”€ kaggle/
>     â””â”€â”€ software/
> ```
> Gitâ€‘ignore large files; treat it as readâ€‘only provenance.

---

## How to make legacy work *useful* for Cultivation

1. **Extract metrics that map to current goals**
   *Example:* Your Kaggle â€œpredict heartâ€‘rate variabilityâ€ model â†’ pull final leaderboard RMSE and any featureâ€‘engineering tricks; cite them in `docs/4_analysis/legacy/` as a benchmark for the new runningâ€‘VOâ‚‚ project.

2. **Harvest proven code snippets**
   - Data loaders, crossâ€‘validation wrappers, LightGBM tuning grids, etc.
   - Relocate to `scripts/common/` with tests so current CI keeps them alive.

3. **Convert lessons into *design constraints* or *requirements***
   - If a past project taught you â€œnever trust early stopping without a holdâ€‘out fold,â€ note that in `docs/2_requirements/testing-requirements.md`.

4. **Tag legacy analyses clearly**
   - Prefix filenames with `legacy_` or nest them under `legacy/` so future readers know these results donâ€™t come from the new integrated pipeline.

5. **Set up CI to ignore legacy folders**
   - Add path filters so broken notebooks in `archive/` donâ€™t fail the build.

---

## Decision checklist

| Question | If **YES** | If **NO** |
|----------|------------|-----------|
| Will I reâ€‘use this model/data in the next 12Â months? | Pull into `data/legacy/` + write a short analysis. | Leave in an external repo; just hyperlink it. |
| Does this code teach a unique lesson that informs the roadmap? | Summarise in `docs/4_analysis/legacy/`. | Skip or just note briefly in background. |
| Is the repository huge (>Â 500Â MB) or has exotic dependencies? | Keep outside; use a submodule or ZIP link. | Safe to archive inside `archive/`. |

---

### TL;DR
*Archive the bulky stuff, surface the insights.*
- **Archive or link** full notebooks / repos so you can reproduce them.
- **Write concise â€œwhat we learnedâ€ reports** in `docs/4_analysis/legacy/` following the same template as new analyses.
- **Promote only reusable utilities** into `scripts/` and protect them with CI.

That way the Cultivation project stays lean, coherent, and forwardâ€‘focusedâ€”while still harvesting every drop of value from your Kaggle glory days and past software adventures. ğŸš€
