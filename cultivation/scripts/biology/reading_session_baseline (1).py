# -*- coding: utf-8 -*-
"""reading_session_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BiASf1H_ibSpHgDWyiFZrDBFCiKXJdpV

---

#### **Cell 1 ‚Äî Markdown**

> # üìñ Instrumented Reading Session
>
> First cut at a notebook that starts a reading session, logs raw telemetry events (page-turns, scrolls, annotations, eye-tracking\*, etc.), and closes with a lightweight analysis.
> \*(*Starred items are placeholders until hardware hooks exist.)*

---
"""

# ======================================================================
# Cell [1]  ‚Äì  Initialise reading-session database
# ======================================================================
import pathlib, sqlite3, json, time, uuid, pprint, datetime as dt
from functools import partial

# -------------------------------------------------------------
# Detect repo root (works in both scripts & Jupyter notebooks)
# -------------------------------------------------------------
def detect_repo_root() -> pathlib.Path:
    try:
        return pathlib.Path(__file__).resolve().parents[2]   # script
    except NameError:                                        # notebook
        cwd = pathlib.Path.cwd().resolve()
        for parent in [cwd] + list(cwd.parents):
            if (parent / ".git").exists() or (parent / "cultivation").exists():
                return parent
        return cwd                                           # fallback

REPO_ROOT = detect_repo_root()
print("üìÇ  Repo root:", REPO_ROOT)

# -------------------------------------------------------------
# Paths for literature subsystem
# -------------------------------------------------------------
LIT_DIR     = REPO_ROOT / "literature"
DB_PATH     = LIT_DIR / "db.sqlite"
SCHEMA_PATH = LIT_DIR / "events_schema.sql"

LIT_DIR.mkdir(exist_ok=True, parents=True)

# -------------------------------------------------------------
# Open SQLite & ensure schema
# -------------------------------------------------------------
conn = sqlite3.connect(DB_PATH)

INLINE_SCHEMA = """
CREATE TABLE IF NOT EXISTS sessions (
    session_id   TEXT PRIMARY KEY,
    paper_path   TEXT,
    started_at   INTEGER,
    finished_at  INTEGER
);
CREATE TABLE IF NOT EXISTS events (
    event_id       INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id     TEXT,
    timestamp_ms   INTEGER,
    event_type     TEXT,
    payload_json   TEXT,
    FOREIGN KEY(session_id) REFERENCES sessions(session_id)
);
CREATE INDEX IF NOT EXISTS idx_events_session_ts
    ON events(session_id, timestamp_ms);
"""

# Prefer external schema file if it exists; otherwise fall back to the inline string
try:
    with open(SCHEMA_PATH) as f:
        conn.executescript(f.read())
except FileNotFoundError:
    conn.executescript(INLINE_SCHEMA)

print(f"‚úÖ  SQLite ready  ‚Üí  {DB_PATH.relative_to(REPO_ROOT)}")

def now_ms() -> int:
    return int(time.time() * 1000)

def insert_event(session_id: str, event_type: str, payload: dict | None = None):
    """Log a single telemetry event."""
    conn.execute(
        "INSERT INTO events (session_id, timestamp_ms, event_type, payload_json) "
        "VALUES (?, ?, ?, ?)",
        (session_id, now_ms(), event_type, json.dumps(payload or {})),
    )
    conn.commit()

def start_session(paper_path: str) -> str:
    """Open a new reading session row & return the UUID."""
    session_id = str(uuid.uuid4())
    conn.execute(
        "INSERT INTO sessions (session_id, paper_path, started_at, finished_at) "
        "VALUES (?, ?, ?, NULL)",
        (session_id, paper_path, now_ms()),
    )
    conn.commit()
    print(f"üìö  Session {session_id[:8]}‚Ä¶ started for", paper_path)
    return session_id

def finish_session(session_id: str):
    conn.execute(
        "UPDATE sessions SET finished_at = ? WHERE session_id = ?",
        (now_ms(), session_id),
    )
    conn.commit()
    print("üèÅ  Session closed.")

# ‚îÄ‚îÄ‚îÄ Replace with the PDF you're actually reading ‚îÄ‚îÄ‚îÄ
PAPER_PATH = REPO_ROOT / "papers" / "example_physiology_2024.pdf"

session_id = start_session(str(PAPER_PATH.relative_to(REPO_ROOT)))

# Optional convenience: wrap the event-logger so we don‚Äôt keep re-typing the ID
log = partial(insert_event, session_id)

# ---- In a real UI these calls fire from callbacks ----
log("page_turn", {"page": 1})
time.sleep(0.8)
log("page_turn", {"page": 2})
time.sleep(1.3)
log("annotation", {"page": 2, "text": "Interesting VO2 equation"})
time.sleep(0.4)
log("scroll", {"delta_px": 450})

print("4 events written.")

finish_session(session_id)

import pandas as pd

df = (
    pd.read_sql(
        "SELECT * FROM events WHERE session_id = ?", conn, params=(session_id,)
    )
    .assign(
        ts=lambda d: pd.to_datetime(d.timestamp_ms, unit="ms"),
        seconds_since_start=lambda d: (d.ts - d.ts.min()).dt.total_seconds(),
    )
    .set_index("ts")
    .sort_index()
)

print("\nüîé  Raw event timeline")
display(df[["event_type", "payload_json", "seconds_since_start"]])

print("\n‚è±Ô∏è  Session duration (s):", df.seconds_since_start.max())
print("üî¢  Number of page-turns  :", (df.event_type == "page_turn").sum())
print("üìù  Annotations added     :", (df.event_type == "annotation").sum())

# Collect high-level stats & persist them for downstream analytics
metrics = {
    "session_id": session_id,
    "pages_viewed": (df.event_type == "page_turn").sum(),
    "annotations": (df.event_type == "annotation").sum(),
    "duration_s": round(df.seconds_since_start.max(), 1),
    "timestamp": now_ms(),
}

conn.execute(
    """
    CREATE TABLE IF NOT EXISTS reading_stats (
        session_id   TEXT PRIMARY KEY,
        pages_viewed INTEGER,
        annotations  INTEGER,
        duration_s   REAL,
        timestamp_ms INTEGER
    )
"""
)
conn.execute(
    "INSERT OR REPLACE INTO reading_stats "
    "(session_id, pages_viewed, annotations, duration_s, timestamp_ms) "
    "VALUES (:session_id, :pages_viewed, :annotations, :duration_s, :timestamp)",
    metrics,
)
conn.commit()

print("üìà  Reading-stats table updated.")
pprint.pp(metrics)

"""Thought for a few seconds


Below are **four more self-contained cells** that complete a ‚Äúminimum-viable, instrumented reading session‚Äù notebook.
Save the notebook at

```
cultivation/notebooks/literature/reading_session_baseline.ipynb
```

(or wherever fits your repo map).
The cell numbers are only a suggestion‚Äîyou can re-order or merge them as you iterate.

---

### **Cell \[2] ‚Äî Helper wrapper to log common UI events**

```python
# ------------------------------------------------------------------
# Helper lambdas so notebook code stays readable
# ------------------------------------------------------------------
PAGE    = lambda n: log("page_turn", {"page": n})
SCROLL  = lambda dy: log("scroll", {"delta_px": dy})
NOTE    = lambda text, page=None: log("annotation", {"page": page, "text": text})

# Example usage inside the session ‚§µÔ∏é
```

---

### **Cell \[3] ‚Äî (Optional) ‚ÄúSimulated read‚Äù loop for quick tests**

```python
# ------------------------------------------------------------------
# Demo: simulate a three-page skim with tiny delays
# ------------------------------------------------------------------
import random, time

for p in (1, 2, 3):
    PAGE(p)
    time.sleep(random.uniform(0.4, 0.9))

SCROLL(300)
NOTE("Need to re-read the metabolic scaling derivation", page=3)

print(f"üìë  Simulated reading events appended (session {session_id[:8]}‚Ä¶)")
```

*(Delete this cell once you wire real UI callbacks.)*

---

### **Cell \[4] ‚Äî Close session & derive per-session metrics**

```python
# ------------------------------------------------------------------
# Finish session & compute metrics
# ------------------------------------------------------------------
finish_session(session_id)                # idempotent if already called

import pandas as pd

events = (
    pd.read_sql("SELECT * FROM events WHERE session_id = ?", conn,
                params=(session_id,))
      .assign(ts=lambda d: pd.to_datetime(d.timestamp_ms, unit="ms"))
      .sort_values("ts")
)
events["since_start_s"] = (events.ts - events.ts.min()).dt.total_seconds()

metrics = {
    "session_id"  : session_id,
    "pages_viewed": int((events.event_type == "page_turn").sum()),
    "annotations" : int((events.event_type == "annotation").sum()),
    "duration_s"  : round(events.since_start_s.max(), 1),
    "timestamp"   : now_ms(),
}

display(events[["ts", "event_type", "payload_json", "since_start_s"]])
print("\nüßÆ  Session metrics", metrics)
```

---

### **Cell \[5] ‚Äî Persist metrics & show aggregate dashboard stub**

```python
# ------------------------------------------------------------------
#  Persist and preview aggregated stats
# ------------------------------------------------------------------
conn.execute('''
CREATE TABLE IF NOT EXISTS reading_stats (
    session_id    TEXT PRIMARY KEY,
    pages_viewed  INTEGER,
    annotations   INTEGER,
    duration_s    REAL,
    timestamp_ms  INTEGER
);''')

conn.execute('''
INSERT OR REPLACE INTO reading_stats
   (session_id, pages_viewed, annotations, duration_s, timestamp_ms)
VALUES (:session_id, :pages_viewed, :annotations, :duration_s, :timestamp);
''', metrics)
conn.commit()

# Quick glance at your historical reading behaviour
summary = pd.read_sql('''
    SELECT
        date(timestamp_ms/1000,'unixepoch')   AS date,
        COUNT(*)                               AS sessions,
        SUM(pages_viewed)                      AS pages,
        SUM(annotations)                       AS notes,
        SUM(duration_s)/60.0                   AS minutes
    FROM reading_stats
    GROUP BY date
    ORDER BY date DESC
    LIMIT 7;
''', conn)

print("üìä  Past-week reading summary")
display(summary)
```

---

## How to iterate rapidly

1. **Add real callbacks**
   *If you‚Äôre using PyMuPDF, PDF.js, or a simple Tkinter viewer,* replace the simulation in Cell \[3] with real event hooks that call `PAGE()`, `SCROLL()`, `NOTE()`.

2. **Extend the schema**
   Need keystroke latency, eye-tracking, or comprehension-quiz scores?
   *Just add columns or a new table*‚Äîthe helper functions will still work because they store free-form JSON.

3. **Batch ‚Üí analytics pipeline**
   Down-the-road, a nightly job can query `reading_stats`, compute moving averages or novelty scores, and push them into the **cognition channel** of your Potential Œ† model.

Feel free to tweak filenames and paths; the cell logic is path-agnostic as long as `detect_repo_root()` can locate the repo. Happy instrumented reading!

**Considering deep dive metrics**

User wants a deeper dive into metrics during reading. I'm considering a range of measurable signals, like time spent per page, scrolling, highlight counts, and note-taking frequency. We can also consider cognitive metrics like self-reported retention and reading comprehension. Since we don't have specialized hardware, I can look at indicators like keystrokes, navigation patterns, or flashcard creation. I'll think through ways to observe behavioral, cognitive, engagement, and complexity metrics to build a comprehensive profile.


Thought for 6 seconds


Below is a **menu of every signal you can plausibly capture while a PDF is open in front of the user, ranked by cost ‚Üî value and mapped to a concrete capture method**.
Pick the subset that matches your privacy comfort-zone and hardware budget; the schema shows where each datum lands so downstream analytics stay trivial.

---

## 0. Legend ‚Äî Four tiers of ‚Äúhow hard is it?‚Äù

| Emoji | Tier                | Capture friction                                | Extra HW / permissions?     |
| ----- | ------------------- | ----------------------------------------------- | --------------------------- |
| üü¢    | **Core** *(‚Äúfree‚Äù)* | pure software; terminal prompts or OS hooks     | none                        |
| üü°    | **Medium**          | browser / desktop plug-in or extra CLI wizardry | no new hardware             |
| üü†    | **Advanced**        | webcam, key-logging (local), sentiment APIs     | camera / mic / higher trust |
| üî¥    | **Frontier**        | eye-tracker, EEG, HRV, galvanic skin            | dedicated devices           |

---

## 1. Observable Metrics Catalogue

| ID      | Metric                                                      | Tier    | Capture hook (example)                                                        | Column(s) & type                         |
| ------- | ----------------------------------------------------------- | ------- | ----------------------------------------------------------------------------- | ---------------------------------------- |
| **T1**  | **Session time** (seconds)                                  | üü¢      | `start_ts`, `end_ts` in SQLite                                                | `minutes_spent INT`                      |
| **T2**  | **Page turns / scroll events**                              | üü°      | PDF.js viewer emits `pagechange` event via WebSocket to CLI                   | `page_count INT`, `scroll_events INT`    |
| **T3**  | **Revisit rate** (pages re-opened ‚â• 3 s after first view)   | üü°      | same event log; compute offline                                               | `revisit_ratio REAL`                     |
| **T4**  | **Note count** (bullets added to MD note)                   | üü¢      | `fswatch` or VS Code extension counts `-` lines                               | `markdown_notes INT`                     |
| **T5**  | **Highlight count / density**                               | üü°      | Poppler/PDF.js ‚Äúadd highlight‚Äù event; fallback = Count `==>` markers you type | `highlights INT`                         |
| **T6**  | **Keystroke bursts** (typing cadence during note-taking)    | üü°      | `readline` hook in CLI; aggregate bursts/min                                  | `keystrokes_per_min REAL`                |
| **T7**  | **Self-rated comprehension** (0-5)                          | üü¢      | CLI prompt at `paper stop`                                                    | `comprehension INT`                      |
| **T8**  | **Self-rated novelty** (0-1)                                | üü¢      | prompt                                                                        | `novelty REAL`                           |
| **T9**  | **Summary length** (words you type in ‚ÄúTL;DR‚Äù)              | üü¢      | regex on MD file                                                              | `summary_words INT`                      |
| **T10** | **Summary cohesion** *(NLP score)*                          | üü°      | run `textstat` or BERT coherence on TL;DR                                     | `summary_coherence REAL`                 |
| **T11** | **Flashcards generated**                                    | üü¢      | count new YAML nodes in `inbox.yaml`                                          | `flashcards INT`                         |
| **T12** | **Immediate recall** (mini-quiz)                            | üü¢      | 3 auto-generated Qs ‚Üí CLI; measure accuracy & latency                         | `recall_acc REAL`, `recall_latency REAL` |
| **T13** | **Delayed recall** (24 h FSRS result)                       | üü°      | parse Flash-Memory reviews for this paper‚Äôs tag                               | `delayed_recall REAL`                    |
| **T14** | **Reading efficiency** (comprehension √∑ minutes)            | derived | computed nightly                                                              | `efficiency REAL`                        |
| **T15** | **Paper complexity** (Flesch, token count, fig/table count) | üü¢      | run NLP on PDF text once at `paper add`                                       | stored in `papers` table                 |
| **T16** | **Emotion / sentiment drift**                               | üü†      | webcam + FER model (face-api) every 10 s                                      | `avg_sentiment REAL`                     |
| **T17** | **Eye-fixation entropy**                                    | üî¥      | Tobii 5 eye-tracker ‚Üí CSV                                                     | `fix_entropy REAL`                       |
| **T18** | **Heart-rate variability** while reading                    | üî¥      | Garmin / Polar BLE stream during session                                      | `hrv_mean REAL`                          |

*(All columns nullable so you can mix tiers without schema changes.)*

---

## 2. Capturing the Streams

### 2.1 Minimal CLI + Mark-I Viewer (üü¢ Tier only)

1. **Launch**: `paper start PID`
2. CLI opens PDF with default OS viewer **and** calls:

   ```bash
   npx live-md literature/notes/PID.md   # opens split-pane editor
   ```
3. On `paper stop PID` it:

   * closes timer
   * asks *T7*, *T8*
   * fires 3 ‚Äúflash recall‚Äù Qs *(T12)*
   * counts TL;DR lines and notes (#T4, #T9)
   * writes all fields to SQLite

No browser plugin, no JS, < 150 LoC.

### 2.2 Adding üü° Metrics

* Swap viewer ‚Üí local **PDF.js** served by the CLI (`fastapi` tiny server).
* JS on `pagechange` posts `{pid, page_no, ts}` to `/event`.
* A second endpoint logs ‚Äúhighlight‚Äù clicks.
* Same server watches the MD note via `chokidar` for bullet inserts.

### 2.3 üü† / üî¥ Metrics

All optional; implemented as plug-in threads that publish to the same `/event` endpoint:

* **Webcam sentiment** ‚Üí JSON `{pid, t, sentiment_score}`
* **Eye-tracker** ‚Üí fixations as `(x, y, duration_ms)`
* **HRV** ‚Üí BLE to MQTT to `/event`

Each writer gets a *paper-session token* so later aggregation is trivial.

---

## 3. Nightly Aggregation Job (`paper stats`)

```python
SELECT
    strftime('%Y-%W', end_ts)          AS iso_week,
    COUNT(DISTINCT paper_id)           AS papers_read,
    SUM(minutes)                       AS minutes_spent,
    AVG(comprehension)                 AS avg_comp,
    AVG(novelty)                       AS avg_nov,
    SUM(flashcards)                    AS fc_created,
    AVG(efficiency)                    AS eff,
    AVG(recall_acc)                    AS recall24h
FROM sessions
LEFT JOIN (
    SELECT session_id,
           SUM(CASE WHEN event='flashcard_created' THEN 1 END) AS flashcards
    FROM events GROUP BY session_id
) USING(session_id)
WHERE end_ts NOT NULL
GROUP BY iso_week
```

This result dataframe ‚Üí `reading_stats.parquet`; your Potential Œ† ingests it unmodified.

---

## 4. Example Derived KPIs

| KPI                      | Formula                                        | Insight                                              |
| ------------------------ | ---------------------------------------------- | ---------------------------------------------------- |
| **Eff-Score**            | `(avg_comp * avg_nov) / minutes_spent`         | Are you *effectively* reading or doom-scrolling?     |
| **Engagement Depth**     | `(highlights + notes) / paper_length_tokens`   | Paper too dense? Or you‚Äôre skimming?                 |
| **Retention Velocity**   | `delayed_recall / minutes_spent`               | Bang-for-buck of each session                        |
| **Cognitive Load Index** | `scroll_events / minutes_spent` (‚Üë = skimming) | High load may correlate with coding fatigue next day |

These KPIs become features in synergy or causal models; no extra work later.

---

## 5. Storage Cheat-sheet

```
literature/
‚îÇ
‚îú‚îÄ‚îÄ pdf/                  -- raw files
‚îú‚îÄ‚îÄ notes/                -- markdown notes
‚îú‚îÄ‚îÄ PaperDB.sqlite        -- papers & sessions
‚îú‚îÄ‚îÄ events.sqlite         -- (optional) low-level events for üü°üü†üî¥
‚îî‚îÄ‚îÄ reading_stats.parquet -- nightly aggregate   <-- Potential engine reads
```

All scripts point to `$LIT_DIR` env var; CI uses a small sample DB.

---

## 6. Privacy & Safety Knobs

* **Tier toggle** in config (`tier = core | medium | advanced | frontier`).
* Redact webcams by default; advanced tiers hash video frames ‚Üí features, discard raw.
* All sensors write **locally**; nothing leaves the machine unless you push to Git.

---

## 7. Quick ‚Äúnext sprint‚Äù plan

| Time      | Milestone                                                                                  |
| --------- | ------------------------------------------------------------------------------------------ |
| **Day 1** | Expand CLI to log `pagechange` from PDF.js ‚ûú events DB (`T2‚ÄìT3`).                          |
| **Day 2** | Add TL;DR length + NLP cohesion scorer (`T9‚ÄìT10`).                                         |
| **Day 3** | Integrate 3-question recall prompt & latency timer (`T12`).                                |
| **Day 4** | Nightly cron converts sessions + events to `reading_stats.parquet`; update Potential yaml. |
| **Day 5** | CI test: synthetic session row + assert KPI columns non-null.                              |

Everything else (webcam, eye-tracker) can wait until you see ROI.

---

### Take-away

You can turn **‚ÄúHere‚Äôs a PDF‚Äîread it‚Äù** into **rich, quantitative telemetry** with nothing but:

1. one tiny CLI,
2. an embedded PDF.js page-change hook,
3. a nightly SQL ‚Üí Parquet roll-up.

Every higher-fidelity metric (eye-tracking, HRV) slots into the exact same *events* table later, so today‚Äôs schema will never block tomorrow‚Äôs ambition.

Thought for 4 seconds


Below is a **‚Äúsketch-to-code‚Äù software architecture** that lets you go from **idea ‚Üí running in a single Jupyter notebook in < 10 minutes**, but still scales into a clean package & CLI when you‚Äôre ready.

---

## 1. 30-second birds-eye diagram

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  pagechange / highlight / notes
‚îÇ  PDF.js front ‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  (JSON events)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ WebSocket               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚ñº                         ‚îÇ  event_sink  ‚îÇ  ‚ûú  SQLite ‚ûä
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îÇ   CLI ‚Äúpaper‚Äù cmd  ‚îÇ start/stop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫  sessions table ‚ûã
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚ñ≤
        ‚îÇ                         ‚îÇ nightly cron
        ‚ñº                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Jupyter notebook  ‚îÇ ‚óÑ‚îÄ‚îÄ ‚îÇ  stats_aggregator   ‚îÇ  ‚ûú  reading_stats.parquet ‚ûå
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

‚ûä **Raw events** (JSON) ‚Äì scrolls, highlights, keystrokes
‚ûã **Sessions** ‚Äì one row per reading session, quick metrics
‚ûå **Aggregated KPIs** ‚Äì weekly roll-up that the Potential engine ingests

---

## 2. Repository mini-layout

```
cultivation/
‚îú‚îÄ literature/
‚îÇ  ‚îú‚îÄ pdf/
‚îÇ  ‚îú‚îÄ notes/
‚îÇ  ‚îú‚îÄ db.sqlite            # created at first run
‚îÇ  ‚îú‚îÄ events_schema.sql
‚îÇ  ‚îî‚îÄ reading_stats.parquet
‚îú‚îÄ notebooks/
‚îÇ  ‚îî‚îÄ reading/quick_demo.ipynb   <-- we'll start here
‚îú‚îÄ scripts/
‚îÇ  ‚îî‚îÄ literature/
‚îÇ       ‚îú‚îÄ cli.py          # `paper start/stop` etc.
‚îÇ       ‚îú‚îÄ event_sink.py   # tiny FastAPI or Flask server
‚îÇ       ‚îî‚îÄ stats_aggregator.py
‚îî‚îÄ pyproject.toml
```

---

## 3. Single-file schema (events\_schema.sql)

```sql
CREATE TABLE IF NOT EXISTS sessions (
    session_id     TEXT PRIMARY KEY,
    paper_id       TEXT,
    started_at     INTEGER,
    ended_at       INTEGER,
    self_comp      INTEGER,    -- 0-5
    self_novelty   REAL,       -- 0-1
    tldr_words     INTEGER
);

CREATE TABLE IF NOT EXISTS events (
    id         INTEGER PRIMARY KEY AUTOINCREMENT,
    session_id TEXT,
    ts         INTEGER,
    event_type TEXT,           -- 'page', 'scroll', 'highlight', 'note'
    payload    TEXT            -- JSON blob
);
```

---

## 4. Tiny reference implementation (all runnable in the notebook)

<details><summary>Python snippets (copy-paste)</summary>

```python
# quick_demo.ipynb ‚Äì cell 1  ‚ñ∏ initialise DB
import sqlite3, time, json, uuid, pathlib, pandas as pd
DB = pathlib.Path("../literature/db.sqlite")
conn = sqlite3.connect(DB)
with open("../literature/events_schema.sql") as f:
    conn.executescript(f.read())
```

```python
# cell 2  ‚ñ∏ helpers
SID = str(uuid.uuid4())
def now(): return int(time.time())

def log_event(ev_type, payload=None):
    conn.execute(
        "INSERT INTO events(session_id, ts, event_type, payload) VALUES (?,?,?,?)",
        (SID, now(), ev_type, json.dumps(payload or {}))
    )
    conn.commit()
```

```python
# cell 3  ‚ñ∏ simulate a short ‚Äúreading session‚Äù
paper_id = "2204.12345"
conn.execute(
    "INSERT INTO sessions(session_id, paper_id, started_at) VALUES (?,?,?)",
    (SID, paper_id, now())
)

for page in range(1, 4):
    time.sleep(0.3); log_event("page", {"page": page})
    log_event("scroll", {"delta": 630})
    if page == 2:
        log_event("highlight", {"selection": "interesting equation"})

# finish session
conn.execute(
    "UPDATE sessions SET ended_at=?, self_comp=?, self_novelty=?, tldr_words=? WHERE session_id=?",
    (now(), 4, 0.7, 42, SID)
)
conn.commit()
```

```python
# cell 4  ‚ñ∏ quick KPI inside the notebook
df = pd.read_sql("SELECT * FROM sessions", conn)
show_cols = ["paper_id", "self_comp", "self_novelty", "tldr_words",
             "(ended_at-started_at)/60.0 AS minutes_spent"]
display(df.assign(minutes_spent=lambda d:(d.ended_at-d.started_at)/60).round(2))
```

</details>

*You‚Äôve just proven the schema, event logger & metric extractor in a throw-away notebook cell.*

---

## 5. Moving notebook code ‚Üí reusable modules (fast iteration plan)

| Sprint  | Notebook cell becomes‚Ä¶                | Saved into                       | Extra test                           |
| ------- | ------------------------------------- | -------------------------------- | ------------------------------------ |
| **S-0** | Hand-rolled SQL & loops               | notebook only                    | none                                 |
| **S-1** | `SessionRecorder` dataclass           | `scripts/literature/recorder.py` | `pytest` ‚Äì insert+select             |
| **S-2** | `stats_aggregator.aggregate_weekly()` | `stats_aggregator.py`            | assert correct ISO-week              |
| **S-3** | CLI wrapper around recorder           | `cli.py`                         | CLI e2e with `subprocess.run`        |
| **S-4** | PDF.js WebSocket ‚Üí `event_sink.py`    | small FastAPI app                | integration test with aiohttp client |

Because all logic is pure **Python+SQLite**, every step can be executed (and hot-reloaded with `autoreload` magic) from the notebook before turning into library code.

---

## 6. Development ‚Äúinner loop‚Äù

1. **Design a new metric** in the notebook: query raw `events`, compute, visualise.
2. **Copy the clean snippet** into `stats_aggregator.py`; add a column to `reading_stats.parquet`.
3. **Write a 3-line test** to confirm aggregation survives edge cases.
4. **Run `pytest -q`** inside the notebook (`!pytest -q tests/test_stats.py`) ‚Üí green.
5. **Promote to CLI** or cron job.

You never break out of the notebook until *after* the metric works.

---

## 7. Where PDF.js fits (2-line summary)

*Spin a tiny FastAPI on `localhost:9004/paper/<SID>` that serves the PDF.js viewer.*

```python
@app.websocket("/ws/{sid}")
async def ws(websocket, sid):
    await websocket.accept()
    while True:
        data = await websocket.receive_json()
        log_event(data["type"], data)           # re-use notebook helper
```

Open that URL in a browser tab; the JS side sends `pagechange` events automatically.

---

## 8. What‚Äôs immediately testable on CI?

| Job               | File                     | Assertion                           |
| ----------------- | ------------------------ | ----------------------------------- |
| **unit-db**       | `tests/test_schema.py`   | all tables exist after script runs  |
| **unit-recorder** | `tests/test_recorder.py` | inserts ‚Üí 1 row in `sessions`       |
| **agg-smoke**     | `tests/test_agg.py`      | aggregator writes non-empty Parquet |

Each job is < 5 s, so iteration remains instant.

---

## 9. Ready-made ‚Äúnext TODO‚Äù list

1. **Commit the schema & helper notebook.**
2. Turn the notebook‚Äôs helper functions into `scripts/literature/recorder.py`.
3. Add `pytest` skeleton & stub CI workflow (`ci-literature.yml`).
4. Decide whether you want the üü¢ core only or also implement the üü° page-event layer now.
5. Celebrate‚Äîyour reading telemetry stack is alive and explorable from Jupyter **today**.

---

### TL;DR

*Start in a notebook with six lines of SQLite & pandas; everything else is just lifting that code into tiny scripts and wiring a one-page PDF.js viewer. The same schema powers fast local experimentation **and** long-term, CI-driven growth.*
"""

