# -*- coding: utf-8 -*-
"""commit_metrics_prototyping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qH5pbBtncA0yzeVtG0sKWgXRYC4449oI
"""



"""# 🛠️ Commit-Metrics Prototyping Notebook

> **Goal:** Rapid-fire, throw-away environment to explore how to extract commit-level
> metrics (LOC churn, cyclomatic complexity, entropy, lint errors, coverage %) from any
> Git repository.  
> Once the code stabilises, we’ll lift it into `scripts/software/commit_metrics.py`
> inside the Cultivation repo and wire it into CI.

---

## 🔗 Reference design docs

| Topic | Doc |
|-------|-----|
| High-level architecture | `docs/3_design/architecture_overview.md` |
| Component catalogue (ETL_S) | `docs/3_design/design_overview.md` § 2 |
| Math stack for code metrics | `docs/2_requirements/math_stack.md` (row “Software-Engineering Metrics”) |
| Task-flow hook | `docs/2_requirements/task_master_integration.md` |
| Formal proof hook | `docs/2_requirements/lean_guide.md` |

---

## 📓 Notebook roadmap

1. **Environment prep** – install GitPython, Radon, Ruff, pandas, pyarrow.  
2. **Clone target repo** – keep it in `/content/repo`.  
3. **Commit list** – SHA, author, date.  
4. **LOC delta** – parse `git show --numstat`.  
5. **Cyclomatic complexity** – Radon per commit checkout.  
6. **Entropy of change** – Shannon on touched files.  
7. **Lint errors** – Ruff on diff.  
8. **Coverage snapshot** – optional; run tests & parse `coverage.xml`.  
9. **Persist Parquet / CSV** – `commit_metrics.parquet` for later import.  
10. **Quick visual hotspot** – bar/heatmap.  
11. **Export to Drive** – keep results after Colab session.  

*(Feel free to delete or re-order sections as you iterate.)*

---

### 🤝 Collaboration tip

If you share this notebook, remind collaborators **not** to
push large repos or private keys—this Colab VM is ephemeral but  
still visible to anyone with the link.

"""

# %%capture
# ─── Environment prep ─────────────────────────────────────────────────────────
# Lightweight wheels only; avoid pulling full dev tool-chains.
!pip -q install gitpython==3.1.43 radon==6.0.1 ruff==0.4.6 \
                pandas pyarrow tqdm

print("✅  Dependencies installed.  Restart kernel if you upgrade versions.")

# %%capture
# ─── Auth-aware clone / refresh ───────────────────────────────────────────────
import pathlib, subprocess, shlex, getpass, os

REPO_URL  = "https://github.com/ImmortalDemonGod/RNA_PREDICT.git"
REPO_DIR  = "repo"
TOKEN_ENV = "GH_PAT"     # env var name (never print it!)

def _run(cmd, **kw):
    print("📀", " ".join(shlex.quote(c) for c in cmd))
    subprocess.run(cmd, check=True, **kw)

repo_path = pathlib.Path(REPO_DIR)

# Ask for token only if we need to clone for the first time
if not repo_path.exists():
    if TOKEN_ENV not in os.environ:
        os.environ[TOKEN_ENV] = getpass.getpass("🔐  Paste GitHub PAT (read-only): ")

    pat = os.environ[TOKEN_ENV]
    # Embed token in HTTPS URL (OAuth basic auth)
    auth_url = f"https://{pat}:x-oauth-basic@github.com/ImmortalDemonGod/RNA_PREDICT.git"
    print("⬇️  Cloning private repo …")
    _run(["git", "clone", "--filter=blob:none", "--progress", auth_url, REPO_DIR])
else:
    print("🔄 Repo already present – pulling latest commits …")
    _run(["git", "-C", REPO_DIR, "pull", "--ff-only"])

# Show current HEAD
sha = subprocess.check_output(["git","-C",REPO_DIR,"rev-parse","--short","HEAD"]).decode().strip()
print(f"✅  Repo ready at commit {sha}")

# ─── Build base commit list ───────────────────────────────────────────────────
import pandas as pd
from git import Repo

MAX_COMMITS = 500          # tweak for bigger history

repo = Repo("repo")
rows = []
for c in repo.iter_commits("HEAD", max_count=MAX_COMMITS):
    rows.append(
        dict(
            sha=c.hexsha[:7],
            author=c.author.name,
            date=c.committed_datetime,
            message=c.message.splitlines()[0]
        )
    )

commits = pd.DataFrame(rows)
print("Total commits fetched:", len(commits))
commits.head()

MAX_COMMITS_API = 200          # cut history while prototyping
commits = commits.head(MAX_COMMITS_API).copy()

from concurrent.futures import ThreadPoolExecutor, as_completed
import requests, os, tqdm, time

GH_PAT = os.environ["GH_PAT"]
HEADERS = {"Accept": "application/vnd.github+json",
           "Authorization": f"token {GH_PAT}"}
OWNER, REPO = "ImmortalDemonGod", "RNA_PREDICT"

def fetch_stats(sha):
    url = f"https://api.github.com/repos/{OWNER}/{REPO}/commits/{sha}"
    r = requests.get(url, headers=HEADERS)
    r.raise_for_status()
    s = r.json()["stats"]
    return sha, s["additions"], s["deletions"]

loc_add, loc_rm = [], []
with ThreadPoolExecutor(max_workers=10) as pool:
    futures = {pool.submit(fetch_stats, sha): sha for sha in commits.sha}
    for f in tqdm.tqdm(as_completed(futures), total=len(futures),
                       desc="GitHub LOC stats (parallel)"):
        sha, add_, del_ = f.result()
        loc_add.append((sha, add_, del_))

# turn list→dict for fast lookup
add_map = {s:a for s,a,_ in loc_add}
del_map = {s:d for s,_,d in loc_add}

commits["loc_add"] = commits.sha.map(add_map)
commits["loc_rm"]  = commits.sha.map(del_map)
print("✅  Added loc_add / loc_rm (parallel)")
commits.head()

# ── Commit-level code-quality metrics (CC + MI) ──────────────────────────────
import subprocess, statistics, ast, concurrent.futures as cf, tqdm
from radon.complexity import cc_visit
from radon.metrics     import mi_visit

REPO_PATH = "repo"                 # local clone
MAX_WORKERS = 10                   # tweak if Colab throttles CPU

def files_changed(sha):
    """Return list of .py paths touched by commit SHA (add / mod / del)."""
    out = subprocess.check_output(
        ["git", "-C", REPO_PATH, "diff-tree", "--no-commit-id",
         "--name-only", "-r", sha],
        text=True
    )
    return [f for f in out.splitlines() if f.endswith(".py")]

def blob_at_commit(sha, path):
    """Return file text at `sha:path`; None if file absent (e.g. deleted)."""
    try:
        return subprocess.check_output(
            ["git", "-C", REPO_PATH, "show", f"{sha}:{path}"],
            text=True, stderr=subprocess.DEVNULL, errors="ignore"
        )
    except subprocess.CalledProcessError:
        return None          # file doesn’t exist in that commit (deleted)

def cc_mi_of_source(src):
    """(total_CC, mean_CC, MI) for a source string."""
    try:
        blocks = cc_visit(src)
        cc_scores = [b.complexity for b in blocks]
        mi_val    = mi_visit(src, False)           # radon expects *string* here
        return sum(cc_scores), (
            statistics.mean(cc_scores) if cc_scores else 0.0
        ), mi_val
    except SyntaxError:
        return 0.0, 0.0, 100.0                     # treat junk as trivial

def metrics_for_commit(sha):
    py_list = files_changed(sha)
    if not py_list:
        return sha, 0.0, float("nan"), float("nan"), 0

    cc_tot, cc_vals, mi_vals = 0.0, [], []
    for path in py_list:
        src = blob_at_commit(sha, path)
        if src is None:
            continue
        tot, mean, mi = cc_mi_of_source(src)
        cc_tot   += tot
        cc_vals.append(mean)
        mi_vals.append(mi)

    mean_cc = statistics.mean(cc_vals) if cc_vals else float("nan")
    mean_mi = statistics.mean(mi_vals) if mi_vals else float("nan")
    return sha, cc_tot, mean_cc, mean_mi, len(py_list)

# ── run in parallel ─────────────────────────────────────────────────────────
out = []
with cf.ThreadPoolExecutor(MAX_WORKERS) as pool:
    futures = {pool.submit(metrics_for_commit, sha): sha for sha in commits.sha}
    for fut in tqdm.tqdm(cf.as_completed(futures),
                         total=len(futures),
                         desc="Quality metrics"):
        out.append(fut.result())

# turn results → dataframe columns
cc_sum_map   = {sha:ccs for sha,ccs,_,_,_ in out}
cc_mean_map  = {sha:ccm for sha,_,ccm,_,_ in out}
mi_mean_map  = {sha:mim for sha,_,_,mim,_ in out}
file_cnt_map = {sha:cnt for sha,_,_,_,cnt in out}

commits["cc_sum"]   = commits.sha.map(cc_sum_map)
commits["cc_mean"]  = commits.sha.map(cc_mean_map)
commits["mi_mean"]  = commits.sha.map(mi_mean_map)
commits["py_files"] = commits.sha.map(file_cnt_map)

print("✅  Added cc_sum, cc_mean, mi_mean, py_files")
commits.head()

import matplotlib.pyplot as plt
import pandas as pd

# ── prepare dataframe ───────────────────────────────────────────────────────
df = commits.copy()

# handle tz-aware strings → UTC → naïve
df["date"] = (
    pd.to_datetime(df["date"], utc=True)   # force parsing in UTC
      .dt.tz_convert(None)                 # strip tz info for rolling-window index
)

df = df.sort_values("date")
df["loc_net"] = df.loc_add - df.loc_rm

# ── 14-day rolling means ────────────────────────────────────────────────────
window = "14D"
roll = df.set_index("date").rolling(window)

loc_roll  = roll["loc_net"].mean()
cc_roll   = roll["cc_mean"].mean() * 50        # scale CC for same y-axis
ax = loc_roll.plot(label="Net LOC (14-d mean)", figsize=(10,4))
cc_roll.plot(ax=ax, label="Mean CC ×50", color="tab:red")

ax.axhline(0, color="grey", lw=0.5)
ax.set_title("LOC churn vs. Complexity trend (14-day window)")
ax.set_ylabel("Lines of code (net)")
ax.legend(); plt.tight_layout()

# ── Robust CC drift matrix (skips missing blobs) ────────────────────────────
import subprocess, statistics, numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt
from collections import defaultdict
from radon.complexity import cc_visit
import tqdm

def py_files_in_commit(sha):
    out = subprocess.check_output(
        ["git", "-C", "repo", "diff-tree", "--no-commit-id", "--name-only", "-r", sha],
        text=True
    )
    return [f for f in out.splitlines() if f.endswith(".py")]

def cc_of_blob(sha, path):
    """Return total CC for path at sha. None if blob missing / unparsable."""
    try:
        blob = subprocess.check_output(
            ["git", "-C", "repo", "show", f"{sha}:{path}"],
            text=True, errors="ignore"
        )
    except subprocess.CalledProcessError:
        return None                       # file absent in that commit (rename/new file)
    try:
        return sum(b.complexity for b in cc_visit(blob))
    except SyntaxError:
        return None                       # invalid/partial python (e.g. diff markers)

# — gather history —
cc_hist = defaultdict(list)
last_n = 150   # commits window

for sha in tqdm.tqdm(commits.sha.head(last_n), desc="Collecting"):
    for f in py_files_in_commit(sha):
        cc = cc_of_blob(sha, f)
        if cc is not None:
            cc_hist[f].append(cc)

# keep ≥3 datapoints
cc_hist = {k:v for k,v in cc_hist.items() if len(v) >= 3}

# select top movers
delta = {k: v[0] - v[-1] for k, v in cc_hist.items()}
TOP = 30
top = sorted(delta, key=delta.get, reverse=True)[:TOP]

# pad to equal length (NaN = older than history)
max_len = max(map(len, cc_hist.values()))
mat = np.array([cc_hist[f] + [np.nan]*(max_len-len(cc_hist[f])) for f in top])

# — plot —
plt.figure(figsize=(11,6))
sns.heatmap(mat, cmap="rocket_r", cbar_kws={"label": "Cyclomatic complexity"})
plt.yticks(np.arange(len(top))+.5, top, fontsize=8)
plt.xlabel("newest commit → older")
plt.title(f"Complexity drift – top {TOP} movers (last {last_n} commits)")
plt.tight_layout()
plt.show()

# ── Commit-level risk score ────────────────────────────────────────────────
import numpy as np

commits["cc_delta"] = commits.groupby("sha")["cc_sum"].transform(
    lambda s: s.iloc[0]  # we only calculated per-commit total
)
commits["risk"] = (commits.loc_add - commits.loc_rm).abs() * commits.cc_delta.fillna(0)
commits.sort_values("risk", ascending=False).head(10)

# ── Flag commits by risk band and emit JSON for the bot ────────────────
import json, pathlib

def risk_band(r):
    if r > 5_000:   return "🚨 HIGH"
    if r > 1_000:   return "⚠️ MEDIUM"
    return "✅ LOW"

commits["risk_band"] = commits["risk"].apply(risk_band)

# 👉 keep only the last N commits pushed in this CI run
last_push = commits.head(50)[["sha", "author", "message", "risk", "risk_band",
                              "loc_add", "loc_rm", "cc_sum", "cc_mean"]]
pathlib.Path("artifacts").mkdir(exist_ok=True)
last_push.to_json("artifacts/commit_risk.json", orient="records", indent=2)
print("📦  artifacts/commit_risk.json written")
last_push.head(10)

"""**Evaluating notebook against goals**

The user wants a fundamental analysis of their notebook, "commit\_metrics\_prototyping," to see if it aligns with the goals of software development improvement. This notebook extracts metrics like LOC churn, CC, MI, and evaluates risk, but they want to ensure it's addressing their goals for systematic improvement. I'll need to determine if there are areas for enhancement or additional forms of analysis missing in the notebook based on these objectives. Time to review its content thoroughly!

**Analyzing notebook features and gaps**

I need to dive into the notebook features, such as commit list extraction, LOC analysis with GitHub, complexity scoring, and risk indicators. I also have to compare this with the repository's goals for systematic software improvement—things like CI integration, risk assessment, file-level metrics, and heatmaps. I'll also look for areas lacking analysis, like maintainability or test coverage delta. After this, recommendations on enhancements—like integrating code health metrics and enabling concurrency—will be outlined. I should summarize achievements, gaps, and action points.


Thought for 17 seconds


Below is a “gap analysis” that lines your **commit-metrics notebook** up against the repo’s stated objectives for *systematic code-base improvement* (as laid out in the design/road-map docs). I’ve kept it action-oriented: each uncovered need is paired with the smallest practical next step plus a pointer to a longer-horizon upgrade.

| Objective in the design docs                       | What the notebook already delivers                            | Gaps / missed signals                                                                                                  | Quick win (≤ 1 day)                                                                                                                                                       | Strategic follow-up (phase tag)                                                                                                                    |
| -------------------------------------------------- | ------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Surface risky commits before merge**             | • Per-commit *risk* scalar (ΔLOC × ΔCC) <br>• 🚨/⚠️/✅ banding | Risk ignores: <br>▪ lint/typing errors<br>▪ failing tests / coverage dips<br>▪ security or licence scanners            | Add two extra columns:<br>`ruff_err` = # Ruff violations on diff<br>`cov_delta` = coverage% change (grabbed from `coverage.xml`)<br>Reuse same 🚨 threshold logic.        | Wire the risk gate into GitHub Actions so a 🚨 commit posts a blocking status check instead of a Slack-only ping. *(phase: P2 “Control + Causal”)* |
| **Track code-health trends over time**             | 14-day rolling mean of LOC-net + CC                           | No repo-level time-series of MI, lint errors, test failures                                                            | Create a “repo snapshot” row after each notebook run (date, global CC mean, global MI mean, Ruff total); append to `data/metrics_repo.csv`. Plot it in the same notebook. | Spin that CSV into a Grafana dashboard fed by nightly workflow. *(P1 “Dynamics”)*                                                                  |
| **Pin-point architectural hotspots**               | Heat-map of top 30 files by ΔCC in last 150 commits           | Doesn’t weigh by file size → tiny test file with a single `if` blow-up can dominate; can’t drill down into directories | Normalise ΔCC by LOC of the file at that commit. Sort by *relative* explosion.                                                                                            | Aggregate per-directory (“/pipeline/stageD/\*\*”); feed into ADR review to decide refactor phases. *(P2)*                                          |
| **Author coaching / code-review load-balancing**   | None yet                                                      | No author-level aggregates                                                                                             | Group by `author`, sum `risk`, chart top 5 (bar).                                                                                                                         | Expose “risk quota” GitHub check: if author’s 4-week total > quota, flag PR for mandatory senior review. *(P3 “Optimisation”)*                     |
| **Automated quality-gate in CI**                   | Notebook writes `artifacts/commit_risk.json`                  | CI doesn’t act on it; no red/green status                                                                              | Tiny `risk_gate.py` (20 lines) → `exit 1` if any ΔCC × ΔLOC > threshold and band == 🚨. Add to `ci.yml`.                                                                  | Extend gate to block if repo-level CC\_mean rolling-avg worsens three weeks in a row. *(P3)*                                                       |
| **Feedback to Flash-Memory / docs**                | None                                                          | No flashcards generated for “why a commit was risky”                                                                   | When a commit hits 🚨, append a YAML card stub to `outputs/flashcards/yaml/code_smells.yaml` (“Q: Why did commit <sha> violate the risk gate?”).                                | Use the same hook to nudge authors to write a *Lessons learned* snippet in `docs/4_analysis/`. *(P3)*                                              |
| **Synergy engine wants software KPIs (C-channel)** | Exports `risk` but not higher-level devops KPIs               | Missing: cycle-time, PR review latency, change-failure rate                                                            | Emit a second JSON blob `software_kpi_weekly.json` with:<br>• mean PR review hrs<br>• merge frequency<br>• test failure ratio (from CI logs)                              | Map those KPIs into the Π potential function’s cognitive channel (weights tuned monthly). *(P2)*                                                   |

---

### Additional blind-spots & how to close them

| Blind-spot                                                     | Why it matters                                                            | Remedy                                                                                                                                      |
| -------------------------------------------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- |
| **Merge commits are skewing LOC stats**                        | A revert or large merge registers as massive churn with zero real effect. | Skip commits that have more than *N* parents **or** set `loc_net=0` if the commit message matches `^Merge ` and treat them as “structural”. |
| **Binary blobs & notebooks counted as LOC**                    | A `.ipynb` diff can add thousands of “lines” of JSON.                     | Filter `--numstat` to `*.py,*.rs,*.ts,...` or use `linguist` language classification.                                                       |
| **Maintainability Index (MI) on minified test files is noisy** | Short files swing MI violently.                                           | Weight MI contribution by file length or compute MI only on files > 100 LOC.                                                                |
| **Plot readability in dark mode dashboards**                   | Red/blue line colours work, but grey axes vanish in dark Grafana.         | Parametrize matplotlib style once (`rcParams['axes.edgecolor']` etc.) and pass theme via env var.                                           |
| **Notebook runtime on CI**                                     | Current run ≈ 40 s for 500 commits; will blow up on full history.         | Add `MAX_COMMITS` env override (default 500; CI uses 200; nightly scheduled workflow uses ∞ with caching).                                  |

---

### Concrete next steps you can drop in **today**

1. **Add Ruff & coverage deltas**

   ```bash
   pip install coverage
   coverage xml -o coverage.xml    # run after tests
   ```

   ```python
   import xml.etree.ElementTree as ET
   root = ET.parse('coverage.xml')
   cov = float(root.find('.//coverage').attrib['line-rate']) * 100
   commits.loc_add - commits.loc_rm
   ```

   Then:

   ```python
   commits["ruff_err"] = commits.sha.apply(lambda s: count_ruff_errors(s))
   commits["cov_delta"] = commits.sha.apply(lambda s: cov - baseline_cov)
   ```

2. **Tiny `risk_gate.py`**

   ```python
   import json, sys, pathlib, math

   THRESH = 5_000   # tweak
   data = json.load(open('artifacts/commit_risk.json'))
   bad = [r for r in data if r['risk'] > THRESH and r['risk_band'].startswith('🚨')]
   if bad:
       print("🚨  High-risk commits found:", [r['sha'] for r in bad])
       sys.exit(1)
   ```

   And in `ci.yml`:

   ```yaml
   - name: Fail on high risk
     run: python risk_gate.py
   ```

3. **Author-level bar-chart**

   ```python
   (commits.groupby("author")["risk"]
           .sum()
           .sort_values()
           .plot.barh(figsize=(6,4)))
   plt.tight_layout(); plt.savefig("assets/risk_by_author.png")
   ```

4. **Doc backlink**

   In `docs/4_analysis/commit_metrics_weekly.md` add:

   ```markdown
   ![risk_by_author](../assets/risk_by_author.png)
   ```

---

### Suggested road-map placement

* **Quick wins** above slot neatly into **P0 (“Bootstrap”)** — no new maths, only plumbing.
* The KPI export & Π-channel hookup belong to **P2 (“Control + Causal”)** when the synergy engine formalises cross-domain metrics.
* The quota-based reviewer load-balancer fits **P3 (“Optimisation”)** where convex/resource allocation maths kicks in.

---

## TL;DR

Your notebook is already a solid “v1 quality radar” for the repo.
By **widening its lens** (lint errors, coverage, author aggregates) and **deciding how CI consumes its output** (blocking gate, dashboards, flashcard hooks) you’ll cover every remaining requirement in the design docs—without rewriting the core logic.  Feel free to ping me for code snippets or CI YAML when you’re ready to drop the next upgrade.

"""