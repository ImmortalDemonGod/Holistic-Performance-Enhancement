# Absolute Zero: Reinforced Self-play Reasoning with Zero Data

## Paper Information
- **Authors**: Andrew Zhao, Yiran Wu, Yang Yue, Tong Wu, Quentin Xu, Yang Yue, Matthieu Lin, Shenzhi Wang, Qingyun Wu, Zilong Zheng, Gao Huang
- **Year**: 2025
- **Source**: arXiv:2505.03335
- **Primary Category**: cs.LG
- **Tags**: reinforcement_learning, reasoning, self_play, zero_shot

## Key Contributions
1. Introduces Absolute Zero, a new RLVR paradigm where a model learns to propose and solve tasks without external data
2. Presents Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum using code execution for verification
3. Achieves SOTA performance on coding and mathematical reasoning tasks without human-curated examples
4. Demonstrates scalability across different model sizes and compatibility with various model architectures

## Methodology
- Uses reinforcement learning with verifiable rewards (RLVR)
- Implements self-play mechanism for task generation and solution
- Employs code executor for task validation and answer verification
- Leverages unified source of verifiable reward for grounded learning

## Results
- Outperforms existing zero-setting models that use tens of thousands of human-curated examples
- Shows effectiveness across different model scales
- Compatible with various model classes

## Implications
- Addresses scalability concerns in human supervision for AI training
- Provides framework for superintelligent systems to learn beyond human-provided tasks
- Demonstrates potential for self-improving AI systems

## Notes
- [Add your reading notes here]

## Questions
- [Add your questions here]

## Related Papers
- [Add related papers here]

## References
- [Add references here] 