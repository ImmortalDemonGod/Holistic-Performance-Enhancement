Cultivation Project Literature Survey

To inform the design of Cultivation’s components (the Think Tank, Laboratory, Flash-Memory layer, etc.), we surveyed recent research (2020–present) in eight relevant areas. Below is a summary table of 3–5 key papers per topic, followed by detailed summaries and their relevance to Cultivation.

Research Topic	Representative Recent Papers (2020–Present)
1. AI-Assisted Hypothesis Generation & Scientific Discovery	(a) Alkan et al. (2025) – Survey on Hypothesis Generation w/ LLMs ￼ (b) Kulkarni et al. (2025) – Hypothesis Generation/Validation (Methods & Datasets) ￼ (c) Ghafarollahi & Buehler (2024) – SciAgents: Multi-agent AI for Discovery ￼ (d) Zhou et al. (2024) – Hypothesis Generation with LLMs (HypoGeniC) ￼ ￼ (e) Lu et al. (2024) – The AI Scientist: Automated Scientific Discovery ￼
2. Knowledge Representation & Reasoning for Scientific Info	(a) Liu et al. (2024) – Neural-Symbolic Reasoning over Knowledge Graphs (Survey) ￼ (b) Zhu et al. (2023) – LLMs for KG Construction & Reasoning (AutoKG) ￼ (c) Li et al. (2024) – Hierarchical Academic Knowledge Graph (Insight Tree) ￼ (d) Ghafarollahi & Buehler (2024) – SciAgents: Ontological KG + LLM for Discovery ￼
3. Analogical Reasoning & Transfer Learning (Cross-Domain)	(a) Chen et al. (2025) – Analogical Learning (Mateformer for cross-scenario) ￼ (b) Xie et al. (2024) – Analogical Prompting for LLMs (ICLR 2024) ￼ (c) Musker et al. (2024) – LLMs as Analogical Reasoners (Cognitive eval) ￼ ￼ (d) Saha et al. (2024) – Analogical Reasoning on Narratives (ARN benchmark) ￼ ￼
4. Reproducibility, Provenance & Workflow Management	(a) Bellomo et al. (2025) – SciRep Framework for Reproducible Experiments ￼ ￼ (b) Barbosa et al. (2025) – Dataset of Computational Experiments (Repro Benchmarks) ￼ ￼ (c) Beber (2025) – Event Sourcing for Reproducible Computational Biology ￼ (d) Marzolla & D’Angelo (2020) – HLA-Based Simulation Interoperability (DDM) ￼
5. Advanced Spaced Repetition & Personalized Learning	(a) Xiao & Wang (2024) – DRL-SRS: Deep RL for Optimizing Spaced Repetition ￼ (b) Tabibian et al. (2019) – Spaced Repetition Optimization (ML model) ￼ (c) Shen et al. (2024) – Knowledge Tracing Survey (models & open tools) ￼ ￼ (d) Minn et al. (2021) – Personalized Review Scheduling via Bandits (noted in surveys)
6. Explainable AI in Scientific & Complex Systems	(a) Li et al. (2022) – “From Kepler to Newton”: XAI-driven Science Discovery ￼ (b) Mengaldo (2025) – Explainable AI and the Scientific Method (XAI4Science) ￼ (c) Iser (2024) – Automated Reasoning for Explanations in Scientific Discovery ￼ (d) Gomez et al. (2025) – Explainability in Hypothesis Validation (SHAP/LIME) ￼
7. Multi-Scale Modeling & Simulation Interoperability	(a) Wang et al. (2024) – Multi-Scale Simulation of Complex Systems (Survey) ￼ (b) Rak et al. (2023) – Multiscale Modeling + ML in Biomedicine (Perspective) ￼ (c) IEEE HLA Standard (Rev. 2020) – High-Level Architecture for Simulator Interoperability ￼ (d) Schulte et al. (2021) – ML Surrogate Models for Multiscale PDEs (Neural Operators) ￼
8. Human-AI Collaboration in Research & Problem Solving	(a) Akata et al. (2020) – Hybrid Intelligence Agenda (Human+AI Teams) ￼ (b) Gomez et al. (2025) – Human-AI Interaction Patterns Taxonomy (Frontiers) ￼ (c) Fragiadakis et al. (2024) – HAIC Evaluation Framework (Symbiotic AI) ￼ ￼ (d) McDonald (2024) – Interactive Gym: Human-in-the-loop RL Platform ￼

Below, we summarize each topic’s key papers, with notes on how each could inform Cultivation’s design:

1. AI-Assisted Hypothesis Generation and Scientific Discovery

Survey & Taxonomy: Alkan et al. (2025) ￼ provide a comprehensive survey of how Large Language Models (LLMs) can aid hypothesis generation. They review prompting methods through advanced frameworks, propose a taxonomy of approaches, discuss techniques like novelty boosting and structured reasoning to improve hypothesis quality, and cover evaluation strategies ￼. This survey highlights the state of the art and challenges (e.g. integrating multimodal data, human-AI collaboration) in using LLMs for scientific insight. Relevance to Cultivation: It offers a roadmap for the Think Tank component to use LLMs for generating research questions or hypotheses, and emphasizes the need for evaluation and human-in-the-loop refinement (aligning with Cultivation’s collaborative goals).

Methodological Advances: Kulkarni et al. (2025) ￼ survey modern hypothesis generation and validation techniques, including agent-based AI “scientists.” They cite systems like The AI Scientist (Lu et al., 2024) and SciAgents (Ghafarollahi & Buehler, 2024) as exemplars of agentic AI that can autonomously handle parts of the scientific process (experiment design, literature review, even manuscript drafting) ￼. These systems combine methods – e.g. retrieval-augmented generation, knowledge graph reasoning, and causal inference – to produce testable, interdisciplinary hypotheses ￼. They map connections across domains that human scientists might miss, showing how LLM-driven tools can break disciplinary silos ￼. Relevance: This informs the Laboratory (for automated experiment suggestions) and Think Tank (for cross-domain idea generation). Cultivation can draw on these approaches to implement multi-agent reasoning: e.g. an agent that proposes hypotheses using literature networks, another that plans validation experiments.

Automated Discovery Systems: Ghafarollahi & Buehler (2024) – SciAgents ￼. SciAgents is a multi-agent framework that leverages a large ontological knowledge graph plus LLMs and in-situ learning to autonomously generate and refine research hypotheses ￼. In a materials science case study, it uncovered hidden cross-domain relationships and elucidated underlying mechanisms beyond what humans had noticed ￼. It essentially acts as an AI “research team,” iteratively critiquing and improving hypotheses and fetching new data. Relevance: SciAgents demonstrates an architectural blueprint for Cultivation’s Think Tank – combining a knowledge repository (graph-based memory) with collaborative AI “thinkers.” Notably, SciAgents’ open-source implementation (per the authors) could be studied or even reused to provide Cultivation with a scaffold for hypothesis brainstorming that is traceable (via the KG) and up-to-date (via retrieval tools).

LLM-Based Hypothesis Generation Methods: Zhou et al. (2024) – HypoGeniC ￼ ￼. This work focuses on generating hypotheses from data using large language models. It introduces HypoGeniC, which first generates initial hypotheses from a small data sample, then iteratively updates them, using a multi-armed bandit-inspired reward to balance exploration vs. exploitation ￼. In tests on classification tasks and real datasets, LLM-generated hypotheses improved predictive performance significantly over naive prompting and even beat some fully supervised models ￼. The authors released their code and data openly ￼. Relevance: For Cultivation’s Laboratory, this suggests a way to have an AI propose data-driven hypotheses (e.g. “feature X might predict outcome Y”) which can then be experimentally verified. The iterative refinement and reward mechanism could inspire how Cultivation’s AI Think Tank refines its suggestions based on feedback (simulating an experiment loop).

Structured Knowledge Extraction for Hypothesis Generation: Sparks of Science (HypoGen) – Aharoni et al. (2025). (Not in table due to space) This method parses research papers into a structured form: each paper’s Bit (established belief), Flip (the paper’s novel idea challenging that belief), and Spark (core insight) are extracted, along with a chain-of-reasoning ￼ ￼. The authors’ HypoGen system, with publicly available code ￼, can generate new hypotheses by recombining these structured elements. Relevance: This aligns with Cultivation’s Flash-Memory layer – the system could maintain a structured memory of prior literature findings (bits and flips) to inspire new combinations. It also shows how explainable representations of knowledge (the chain-of-reasoning from Bit to Flip) let an AI generate hypotheses that are easier for humans to understand and verify, an ethos Cultivation should adopt for user trust.

In summary, Topic 1 literature indicates that Cultivation’s Think Tank should incorporate: a knowledge-integrated LLM approach for broad hypothesis generation (per surveys  ￼ ￼), multi-agent or iterative frameworks for refining and testing those hypotheses (e.g. SciAgents ￼), and tools to evaluate novelty and validity of AI-proposed ideas. Many works also stress human-AI collaboration (e.g. AI suggests, human filters) – aligning with Cultivation’s goal of keeping a “human in the loop” to guide the AI’s open-ended creativity.

2. Knowledge Representation and Reasoning for Scientific Information

Neural-Symbolic Knowledge Reasoning: Liu et al. (2024) ￼ present a recent survey on reasoning over knowledge graphs (KGs), merging symbolic and neural techniques. They note that traditional symbolic inference struggles with incomplete/noisy data, while Neural-Symbolic AI combines deep learning’s robustness with symbolic logic’s precision ￼. The survey categorizes query types and highlights how the latest large language models can extract and integrate knowledge in unprecedented ways ￼. Crucially, these methods aim to produce interpretable and explainable reasoning on KGs, which is key for scientific domains where trust in AI reasoning is paramount. Relevance: For Cultivation’s knowledge base (the Flash-Memory layer), this suggests using a hybrid approach: represent scientific facts in a graph or logical form, and use neural reasoning (e.g. graph neural networks or Transformers with KG context) to answer complex queries or deduce new relations. The survey provides algorithms Cultivation can leverage to ensure that the AI’s reasoning over stored knowledge remains transparent and verifiable (e.g. via symbolic traces).

LLMs Augmented with Knowledge Graphs: Zhu et al. (2023) ￼ quantitatively evaluated GPT-4 and similar LLMs on both KG construction tasks (extracting entities/relations) and KG reasoning tasks. A key finding is that GPT-4 excels as an “inference assistant” rather than as a raw extractor – i.e. it performed even better in reasoning tasks (like answering questions by traversing a KG) than fine-tuned models, while its information extraction was competent but not flawless ￼. They introduced a Virtual Knowledge Extraction benchmark and AutoKG, a multi-agent system using LLMs plus external tools for automatic KG building and inference ￼. Relevance: This suggests Cultivation should use LLMs primarily to interpret and reason with scientific knowledge (once it’s represented), rather than relying on them alone to build the knowledge base. For example, the Think Tank might use a GPT-4-based agent to reason over a graph of prior results to find connections (“what previous findings combined might imply this new hypothesis?”), while a more structured pipeline (or smaller models) handle curating the knowledge graph. The AutoKG idea – multiple agents that populate and query a knowledge base – could be adapted to Cultivation’s architecture for the knowledge management subsystem.

Academic Knowledge Graphs and Insights: Li et al. (2024) ￼ developed a Hierarchical Tree-Structured Knowledge Graph for academic papers to aid newcomers’ understanding. Instead of a flat similarity-based graph, it captures a logical hierarchy of research issues, showing how one paper builds on or addresses the issues of another ￼ ￼. This hierarchy of “issue resolved” → “issue remaining” creates a map of scholarly knowledge. Relevance: Cultivation’s Flash-Memory could use a similar approach to organize stored information. Rather than just a list of papers or facts, it can maintain a structured ontology of scientific problems and hypotheses – which the Think Tank can traverse when generating new ideas (ensuring it knows what’s been tried or what gaps exist). This work also underscores the need for explainability in knowledge representation: the graph explicitly links why one result is relevant to another, which aligns with Cultivation’s goal of providing context to the user.

Knowledge Graphs for Discovery: (Also mentioned in Topic 1) SciAgents (2024) ￼ is relevant here too. It uses a large-scale ontology (knowledge graph of scientific concepts) as the backbone for multi-agent reasoning. This demonstrates how a well-structured knowledge representation can enable complex reasoning: SciAgents’ agents navigate the KG to propose hypotheses and even explain them by the paths they find between concepts ￼. Relevance: Cultivation should similarly employ a graph-based memory of scientific knowledge, enabling the AI to reason about connections (the Think Tank could find indirect links between two concepts via intermediate nodes, sparking an analogy or hypothesis). Additionally, SciAgents highlights interoperability: it combined data retrieval, LLM reasoning, and knowledge reasoning – Cultivation’s architecture may mirror this by having the Flash-Memory layer expose an API that both the Think Tank and Laboratory agents query for relevant info (ensuring consistent knowledge across components).

In summary, Topic 2 suggests that Cultivation’s knowledge layer be built on an explicit knowledge graph or similar structure to capture scientific information (papers, results, domain facts). Techniques like neural-symbolic reasoning ￼ and LLM-based inference on graphs ￼ will allow the Think Tank to draw new conclusions from stored knowledge (e.g. inferring a hypothesis by analogy to a known pattern). We also learn that maintaining explainability (via symbolic edges or semantic relationships) is crucial – the system should be able to justify why a given prior knowledge was used in reasoning, bolstering user trust. Open-source tools (e.g. the EduKTM library for knowledge tracing or VINE dataset for KGs) and standards (like RDF, or domain ontologies) from these works can jump-start the implementation of Cultivation’s knowledge base.

3. Computational Analogical Reasoning and Cross-Domain Transfer Learning

Analogical Learning Frameworks: Chen et al. (2025) ￼ introduce Analogical Learning (AL) as a general deep learning framework for cross-scenario generalization. Their “Mateformer” architecture learns to make predictions in a new scenario by implicitly referencing similarities to a known scenario ￼. In essence, it retrieves a “reference frame” from one context and applies a relative analogy to another, allowing zero-shot transfer across differing environments. Applied to wireless network localization, their AL method achieved state-of-the-art accuracy and two orders of magnitude lower error than conventional methods, without any retraining in the new scenario ￼. They open-sourced data and code for reproducibility ￼. Relevance: This demonstrates a concrete way Cultivation’s Laboratory might transfer models or hypotheses between domains. For instance, if Cultivation has solved a problem in one field, the Think Tank could use analogical learning to suggest solutions in an unrelated field by mapping the structural relations. The Mateformer’s approach of computing relational representations between current data and embedded reference data could inspire how Cultivation’s AI draws parallels (e.g. between a biological network and a social network phenomenon). It’s also a reminder that open-source implementations are available – Cultivation could potentially integrate the Mateformer model to empower its cross-domain hypothesis suggestions.

Analogical Prompting for Reasoning: Xie et al. (2024) ￼ (ICLR 2024) propose analogical prompting as a novel prompting strategy for large language models. Rather than standard Chain-of-Thought, their method has the LLM generate its own analogous examples or relevant knowledge before solving a problem ￼ ￼. By prompting the model to recall similar solved problems (“think of a related problem and solution”) and high-level schemas, they guide it to leverage analogies the way humans do ￼. This approach outperformed zero-shot and few-shot chain-of-thought on tasks like math word problems and code generation, improving accuracy by ~4% on average ￼. Relevance: For Cultivation’s Think Tank, this suggests incorporating a step where the AI explicitly searches for analogous cases in its memory (or external databases) when faced with a new problem. Instead of producing an answer directly, the Think Tank agent could first retrieve a comparable scenario (from the Flash-Memory layer or a case library) and use it to scaffold the solution. The success of analogical prompting also highlights that no additional training was required – it’s a prompting technique – which Cultivation could adopt on the fly with large models to enhance reasoning performance in complex problem-solving.

Cognitive Evaluation of LLM Analogies: Musker et al. (2024) ￼ ￼ examine whether state-of-the-art LLMs truly perform analogical reasoning in a human-like way. They designed tasks requiring mapping between structured sequences and semantic concepts, and compared GPT-4 and other LLMs to human performance ￼. They found that advanced LLMs can match human success on several analogical tasks, providing evidence that some form of analogy emerges from domain-general training ￼ ￼. However, the models showed differences in how they handle distractions and novel formats, indicating current LLMs’ analogies might be “surface-level” in some cases rather than deep structural mapping ￼ ￼. Relevance: This study provides reassurance that Cultivation’s large-language-model components (like GPT-based agents) have a non-trivial capacity for analogy – an important capability for creative scientific discovery (e.g. finding an analogous solution from another domain). It also cautions that the Think Tank should perhaps incorporate explicit analogical reasoning modules (like the above analogical prompting or knowledge graphs of analogies) to overcome LLMs’ limitations. In Cultivation’s design, human researchers could be prompted to verify or refine the analogies the AI draws, given that LLM analogies might sometimes be superficial. Musker et al. also align with the idea that human cognitive models (e.g. Gentner’s structure-mapping theory) are still valuable – Cultivation might encode such models to evaluate AI-proposed analogies.

Domain-Specific Analogy Benchmarks: Saha et al. (2024) – ARN (Analogical Reasoning on Narratives) created a benchmark for far analogies using short stories ￼ ￼. They report that even with chain-of-thought and examples, the best GPT-3/4 models achieved far below human-level performance, especially on far analogies (structurally similar but surface-dissimilar story pairs) ￼ ￼. This underscores the challenge of analogical transfer when superficial cues are absent. Relevance: For Cultivation, this implies that truly novel, cross-domain insights (which often require “far” analogies) may not emerge from LLMs without additional structure. The project may need to implement specialized analogical reasoning modules or incorporate external knowledge (e.g. semantic relation databases, case-based reasoning systems) to support the Think Tank in drawing deep analogies. It also highlights the importance of evaluation: Cultivation could use such benchmarks or tasks to regularly test and fine-tune its hypothesis-generation analogies, ensuring the system’s suggestions aren’t just correlational but capture a meaningful structural parallel.

In summary, Topic 3’s literature will inform Cultivation’s Think Tank and problem-solving AI to be adept at analogy and transfer. Key takeaways are: (a) use specialized frameworks (like Mateformer) for explicit cross-domain mapping when data is available ￼; (b) leverage prompting techniques that make AI “recall” analogous solutions ￼; and (c) maintain a knowledge base of solved problems and solution patterns so that the AI (or user) can draw comparisons. The research also reminds us that human oversight is vital – many authors stress that AI analogies should be interpretable so humans can confirm if the analogy truly holds. Cultivation’s UI might, for instance, show the user “the AI was inspired by Solution X in domain Y” so the researcher understands and can validate the transfer.

4. Reproducibility, Provenance, and Workflow Management in Computational Science

Reproducibility Frameworks: Bellomo et al. (2025) – SciRep ￼ ￼. This work presents SciRep, a framework to encapsulate computational experiments such that they can be exactly rerun on any machine ￼. SciRep defines an experiment’s code, data, dependencies, environment, and execution steps, then packages it into a capsule that holds “everything necessary to re-execute the experiment” and produce identical results ￼. In an evaluation, SciRep could successfully reproduce ~89% of 18 published experiments from various fields, outperforming other tools (which hit ~61%) ￼. All reproduced runs matched the original results (100% consistency for those that ran) ￼. Relevance: Cultivation’s Laboratory component (which runs simulations/analyses) should incorporate similar packaging of each hypothesis test or experiment. Using an approach like SciRep, the Laboratory could automatically record the exact environment (software versions, data inputs) whenever an AI or human runs an experiment, creating a reproducibility package. This would allow any team member (or even an external reviewer) to rerun the analysis later, a critical feature for scientific trust. SciRep’s principles (capture code + data + commands) can be integrated into Cultivation’s workflow manager backend. Additionally, the fact that SciRep covers multiple domains means Cultivation’s system can be domain-agnostic in managing workflows.

Provenance Datasets and Benchmarks: Barbosa et al. (2025) ￼ ￼ compile a curated dataset of computational experiments across many fields, including detailed metadata on dependencies, configurations, and execution steps required for reproduction ￼. The dataset is structured to represent diverse workflows (from simple scripts to complex multi-language pipelines) and is used as a benchmark to evaluate reproducibility tools ￼ ￼. They documented each experiment uniformly so that tools like SciRep or others can be systematically compared ￼. Relevance: This signals that Cultivation’s team should adopt or contribute to such benchmarks to validate our platform’s reproducibility. Before deployment, we can test Cultivation’s Laboratory by attempting to reproduce these reference experiments, ensuring our system handles various workflow types. Moreover, the standardized metadata schema from this dataset could inform how Cultivation’s system stores experiment provenance: e.g. using a structured format (JSON or RDF) for execution records (data sources, code version, parameters). Having a Flash-Memory of experiments with this metadata would enable advanced queries like “find all experiments that used method X on dataset Y” and help implement features like automated result comparison or provenance tracing in Cultivation.

Provenance via Event Sourcing: Beber (2025) ￼ proposes using event sourcing (a software pattern from enterprise systems) to address reproducibility in computational biology. Event sourcing means recording every change to the system state as an immutable sequence of events (similar to version control or a ledger) ￼. Beber argues that storing the full history of model changes, rather than just final states, yields complete provenance and allows “perfect replication of processes” by replaying the event log ￼. For example, in a simulation, every parameter change or simulation run is an event; re-running those in order reconstructs the entire research workflow. Relevance: Cultivation can adopt an event-sourcing approach in its workflow management: every action the AI or user takes (code executed, hypotheses generated, data transformed) is logged as an event in a timeline. This would give the Flash-Memory layer a fine-grained history of the project’s progress. If something is discovered later to be in error, one could trace back through events to diagnose where it came in. Also, if a user wants to “branch” an experiment, the event log can be forked – similar to how git enables branching code. Implementing this requires some overhead, but the benefit is strong guarantees of reproducibility and an ability to audit the AI’s contributions. Notably, event sourcing aligns with Cultivation’s goal of transparency: a user could inspect the event log to see exactly what sequence of steps led to a result (which is crucial when AI is involved in generating or modifying those steps).

Workflow Interoperability Standards: While older, the IEEE High-Level Architecture (HLA) for distributed simulation is worth noting. It provides a standard for making different simulation components interoperate by managing how they exchange data in runtime (through a Run-Time Infrastructure and Data Distribution Management) ￼. The literature (e.g. Marzolla & D’Angelo 2020) shows how HLA’s Data Distribution Management (matching publishers and subscribers of data regions) can be parallelized on modern hardware ￼. Relevance: If Cultivation’s Laboratory will integrate multiple simulation modules or external tools, using a standard like HLA or the more recent Functional Mock-up Interface (FMI) could ease integration. For example, if one hypothesis test involves a climate model and another a machine learning model, HLA would allow them to run in a co-simulation, synchronizing their time steps. Cultivation might not need full HLA, but adopting its concept of a common interoperability layer will future-proof the platform – users could plug in their own simulators which communicate through this layer. The parallel DDM algorithms ￼ indicate that performance can scale even if many components are exchanging data. In short, to manage complex workflows, Cultivation should use established best practices from the simulation community for orchestrating components and tracking data flow.

In summary, Topic 4 literature underlines that reproducibility and provenance must be core design considerations for Cultivation. Concretely, we will: (a) implement experiment packaging akin to SciRep (perhaps containerize each run with Docker/Singularity) ￼; (b) maintain detailed provenance metadata for each result (possibly via an event log) ￼; and (c) utilize existing standards/benchmarks to validate our approach (ensuring, for example, that if a Cultivation workflow is exported, it could be run by an external reproducibility tool and vice versa). By doing so, Cultivation’s Laboratory will not only help generate discoveries but also ensure those discoveries can be trusted and verified by others – addressing the “reproducibility crisis” head-on with technical solutions.

5. Advanced Spaced Repetition Systems and Personalized Learning Models

Deep Reinforcement Learning for Spaced Repetition: Xiao & Wang (2024) – DRL-SRS ￼. This work treats scheduling of reviews (flashcards, learning items) as a sequential decision problem and uses Deep Q-Networks (DQN) to optimize it. Unlike traditional hand-crafted intervals (SuperMemo, Leitner, etc.), their framework learns the optimal review interval for each item and user state via reinforcement learning ￼. They introduce a Transformer-based memory model to predict recall probabilities over time, which guides the RL agent in scheduling the next review ￼. Experiments show state-of-the-art recall outcomes – e.g. their system achieved higher average recall rates than both standard heuristics and other learned models in multiple settings ￼ ￼. Relevance: For Cultivation’s Flash-Memory layer (the long-term knowledge store for the team), incorporating an intelligent spaced repetition engine can ensure that important information (papers, insights, experimental results) is surfaced to team members at the right intervals to combat forgetting. If Cultivation has a knowledge base of facts or hypotheses, a DRL-SRS approach could personalize which items to remind a researcher (or an AI agent) about. For example, if an AI in the Think Tank “forgets” a relevant paper, a spaced repetition mechanism could prompt it with that knowledge at critical times. The open-source code from this paper (the authors likely released their implementation) provides a starting point to integrate a proven RL scheduler into Cultivation, adapting it to schedule not flashcards per se, but review of scientific knowledge for users and AI alike.

Spaced Repetition Optimization (Foundational): Tabibian et al. (2019) – PNAS ￼. Although slightly before 2020, this highly-cited work is included as it laid groundwork. They formulated memory retention as a stochastic process and used a machine learning model to optimize review intervals for each learner, demonstrating improved learning efficiency over SuperMemo. Code and data were made available, spurring follow-up research. Relevance: This is fundamental to understand for Cultivation’s design of the Flash-Memory layer. It reminds us that any spaced repetition algorithm should be grounded in learning science (e.g. the exponential forgetting curve) and that there are established metrics (predictive retention, memory decay models like half-life regression). Cultivation can use the insights (e.g. the concept of modeling each knowledge item’s “difficulty” and the user’s memory strength parameters) from Tabibian et al.’s model to initialize or guide more advanced approaches like DRL-SRS. Essentially, it means our system won’t rely on arbitrary heuristics – we can quantitatively model and predict how information decays for each user and use that to schedule reminders, thus serving as a “Flash-Memory” that truly augments the team’s collective memory.

Knowledge Tracing for Personalization: Shen et al. (2024) – KT Survey ￼ ￼. Knowledge Tracing (KT) is the task of modeling a student’s evolving knowledge state as they learn, typically to personalize practice or recommendations. This survey covers classical models (e.g. Bayesian Knowledge Tracing), deep learning models like DKT (Deep Knowledge Tracing) and recent variants that incorporate cognitive assumptions or graph structures ￼. Importantly, the authors also released open-source libraries (EduData and EduKTM) for processing education datasets and implementing KT models ￼. Relevance: In Cultivation, while our “learners” are researchers (and the AI itself), the concept still applies: the system should maintain a model of what each team member knows or has seen. For example, if a researcher consistently ignores papers on a certain topic, the system might infer they find it less relevant (or too difficult) and adjust suggestions accordingly. Or if the AI has attempted a certain type of hypothesis multiple times and failed, the system updates the AI’s “knowledge state.” By using KT techniques, Cultivation’s personalization can go beyond spacing timing and also adapt content selection – much like a tutor chooses appropriate next problems for a student. The availability of unified libraries means we could integrate these models relatively easily to maintain user profiles. Over time, the Flash-Memory layer combined with KT could answer questions like “Has person X encountered concept Y before? How well do they seem to understand it?” and then either prompt a review (spaced repetition) or avoid redundancy. This ensures personalized learning and recall for each user in the system.

Other Personalized Learning Models: Research also highlights methods like multi-armed bandits for adaptive content scheduling (e.g. selecting the next experiment or reading for maximum learning), and attentive study tools (some works integrate EEG or eye-tracking to gauge attention in learning – though beyond Cultivation’s current scope, these indicate directions for future personalized UX). Duolingo’s public datasets and competitions on spaced repetition (2020, 2021) have spurred models that combine forgetting curves with student ability estimates. Cultivation can borrow from these by treating each hypothesis or domain skill as something that can be “mastered” by the team. For instance, if the AI repeatedly assists in a certain analysis, a human team member might implicitly learn that skill, eventually not needing AI for it – the system could detect this and adapt how it allocates tasks (a form of personalized team learning model, assigning tasks to whoever is “best learned” in that area). This is a conceptual leap but supported by principles in these papers.

In short, Topic 5 teaches us that Cultivation’s knowledge layer should not be static; it should actively manage the timing and personalization of information exposure. By integrating a spaced repetition scheduler (like DRL-SRS’s Transformer-based policy for scheduling reviews ￼) the Flash-Memory layer ensures key knowledge remains fresh for the team. By maintaining knowledge proficiency models (knowledge tracing) ￼, the system can tailor which suggestions or explanations to give to which user (for example, providing more background context to a junior scientist but not to a senior expert, or vice versa if the senior hasn’t worked in that sub-field). Ultimately, these techniques contribute to Cultivation’s goal of augmenting human intellect: they offload the burden of memorization to the AI and guarantee that both the humans and AI agents get the right information at the right time for maximal long-term learning.

6. Explainable AI (XAI) in Scientific and Complex Systems

XAI as a Driver for Scientific Discovery: Li et al. (2022) – “From Kepler to Newton” ￼. This paper vividly demonstrates a paradigm where explainable AI is used not just to explain a model, but to actually aid in discovering scientific laws. The authors use an interpretable model on astronomical data (Tycho Brahe’s observations) and show that by examining the model’s explanations, one can rediscover Kepler’s laws of planetary motion and even Newton’s law of gravitation ￼. Essentially, the AI produces a model of the data, and because the model is explainable (e.g. via symbolic regression or transparent reasoning), human scientists can interpret those explanations as hypotheses or principles of nature. This highlights the notion of “Explainable AI for Science”, where the focus is on human-AI synergy: the AI might crunch data or fit complex patterns, but the human extracts meaning from AI’s explainable patterns. Relevance: Cultivation’s Think Tank should incorporate XAI methods so that any hypotheses or recommendations the AI provides come with interpretations that domain experts can understand. For instance, if Cultivation’s AI suggests a new catalyst for a chemical reaction, it should also provide an explanation (perhaps a set of feature importances or a simplified rule) that a chemist can evaluate, rather than a black-box output. The success of Li et al.’s approach suggests that coupling AI pattern-finding with scientific reasoning yields results that are both novel and interpretable. Cultivation could, for example, use symbolic regression tools (like the one presumably used to get Newton’s law) to convert numerical patterns into human-readable equations – turning data into candidate scientific models.

Explainable AI and the Scientific Method: Mengaldo (2025) ￼ discusses a conceptual framework called “Interpretability-Guided Knowledge Discovery.” The idea is that while generative AI can sift through data, true scientific discovery still requires interpretable principles. The paper defines Explainable AI for Science as systems where domain experts work with AI that reveals the “principles” it used to make decisions ￼. By examining these AI-derived principles (which might be correlations or rules the model found), scientists can either form new hypotheses or challenge existing ones ￼. Divergent views between AI’s explanation and human expectations can spark investigations – essentially, the explainable components of AI become hypotheses themselves. Relevance: This philosophical stance reinforces that Cultivation should prioritize explainability at every layer – from the Think Tank’s reasoning to the Laboratory’s results. For example, if the Think Tank’s multi-agent system converges on a hypothesis, the system should output not just the hypothesis but why it thinks so (e.g. “Agent A and Agent B found these supporting connections”). These explanations can lead the human team to new insights or spot errors in the AI’s reasoning. The paper suggests that explainability is the bridge between AI pattern recognition and human scientific reasoning. Practically, Cultivation might implement libraries like SHAP or LIME on its ML models to highlight which variables or data points influenced an AI decision ￼. In scientific applications, that could translate to identifying which prior studies or which experimental data points most strongly sway an AI-derived hypothesis – information crucial for users to assess validity.

Automated Reasoning and Explanations: Iser (2024) ￼ explores using automated reasoning (logical inference) to generate and select explanations for AI decisions in a scientific context. The paper proposes a cycle: machine learning produces a model (induction), then automated reasoning translates that model into a formal explanation which serves as a hypothesis, then that hypothesis is tested deductively ￼ ￼. It presents a taxonomy of explanation “selection” – essentially criteria to decide what constitutes a good explanation (e.g. simplicity, broadness, consistency with prior knowledge) ￼. Relevance: For Cultivation, this is directly applicable to how the Think Tank might present its conclusions. We can imagine that once the Think Tank’s AI has a potential hypothesis, it could use a logic engine to derive a set of explanatory statements or conditions that led to that hypothesis. Then, using a knowledge base of what makes a good scientific explanation (perhaps favouring simpler laws, or those consistent with established theory), it can select the best explanation to show the user ￼. This approach also suggests the integration of formal methods (like constraint solvers or theorem provers) within Cultivation. For example, if an AI model suggests a relationship between variables, automated reasoning could verify if that relationship holds under known constraints or if it violates any known laws – thereby explaining by validation or counterexample. Iser’s work aligns with making AI not just a black-box suggestion generator, but part of a scientific dialogue, where it proposes an explanation and then tests it (similar to how a scientist would). Implementing this could mean Cultivation provides a feature where for any output, the user can request “Explain/Derive this,” and the system either produces a symbolic derivation or a logical argument supporting the output.

XAI Techniques in Practice: Many scientific ML applications use techniques like feature importance scores, saliency maps, concept activation vectors, and surrogate models to interpret complex models. For instance, in climate modeling, researchers use XAI to identify which physical factors an AI model deemed important for a prediction. In Cultivation’s Laboratory, when an AI model is used to predict an experimental outcome, techniques like SHAP values could ensure the model’s reasoning aligns with scientific expectations (if not, the scientists know to be skeptical). The survey by Gomez et al. (2025) even notes that current human-AI collaborations in decision-making often rely on “simplistic collaboration paradigms” and more interactive, explanatory exchanges are needed ￼. Thus, Cultivation should facilitate an interactive XAI environment: users should be able to query “Why did the AI suggest this experiment?” and get a clear rationale, possibly through an interface that visualizes the relevant portion of the knowledge graph or the factors considered.

In summary, Topic 6 emphasizes that explainability is not optional in scientific AI – it is essential. For Cultivation, this means every hypothesis generated by the system should come with an explanation that scientists can scrutinize (and even learn new science from, as in Li et al.’s case) ￼. Concretely, we will integrate XAI libraries and possibly a symbolic reasoning module so that the Think Tank’s outputs and Laboratory’s models are interpretable. This will increase user trust and also turn the AI into a true research partner that can point out “here’s why I think this is interesting,” sometimes leading the human to the “aha!” moment. By adopting an XAI-first approach, Cultivation will ensure that it not only generates scientific discoveries but also that those discoveries (and the path to them) are understandable and actionable by human experts.

7. Multi-Scale Modeling and Simulation Interoperability

Survey of Multi-Scale Simulation: Wang et al. (2024) ￼ provide a systematic review of multi-scale modeling techniques for complex systems. They discuss scenarios with clear scale separations (e.g. micro vs. macro) and those with unclear scale boundaries, and categorize objectives of multi-scale simulations into five types (such as bridging temporal scales or spatial scales) ￼. Importantly, they frame multi-scale simulation in terms of integrating domain knowledge and data: some approaches are knowledge-driven (e.g. using known physics across scales) while others use data-driven coupling (machine learning to link scales) ￼. They also enumerate applications ranging from materials science to social systems where multi-scale methods are applied ￼. Relevance: Cultivation’s Laboratory might need to handle multi-scale models – for example, a biological simulation might involve molecular dynamics (micro-scale) and cell-level dynamics (macro-scale) that need to work in concert. This survey can guide how we design the interoperability layer of Cultivation’s simulation management. It suggests that we may need multiple strategies: if the relationship between scales is known (like constitutive equations linking micro to macro), we should encode that knowledge; if it’s unknown, we might employ data-driven surrogates (like neural networks to translate outputs of one scale to inputs of another). The survey’s taxonomy helps ensure we consider all cases. Also, by knowing typical challenges (stability, consistency between scales, computational cost), we can equip Cultivation with features like adaptive resolution – e.g. run coarse models generally, but automatically switch to fine-scale simulation when a hypothesis requires it. The survey likely references software frameworks as well, which we could evaluate for integration (some known ones: MUSCLE, MOOSE, etc., for multi-scale coupling).

Physics-Informed ML for Multiscale Problems: Kashinath et al. (2021) (cited via arXiv/NeurIPS) ￼ discuss how machine learning (like neural operators or multi-fidelity models) can assist traditional multi-scale simulations. For instance, a physics-informed neural operator can learn the coarse-to-fine mapping in PDE simulations, enabling accurate approximations of high-resolution physics without resolving every detail explicitly ￼. Relevance: For Cultivation, this means our Laboratory could incorporate surrogate models to connect scales. Suppose the Think Tank poses a hypothesis requiring an expensive multi-scale simulation; the system could instead use a trained neural network that “emulates” the fine-scale model based on coarse outputs – drastically speeding up the experiment cycle. The concept of Neural Operator (like the EquiNO mentioned) allows for generalizing across parametric conditions, which is ideal for hypothesis testing where parameters vary. Incorporating such ML surrogates will make Cultivation more efficient. However, the challenge is ensuring the surrogate is accurate and doesn’t introduce artifacts – tying back to reproducibility and XAI: we’d need to validate the surrogate and possibly explain its approximations.

Interoperability Frameworks: High-Level Architecture (HLA) – IEEE 1516 is a cornerstone for simulation interoperability ￼. It enables different simulation modules (which they call federates) to exchange data in a synchronized way through a Run-Time Infrastructure (RTI). Key to HLA is a publish-subscribe mechanism for data and a time management system so that distributed components remain consistent. Modern uses of HLA (as noted by Marzolla & D’Angelo) have parallelized parts like Data Distribution Management to handle large numbers of entities efficiently ￼. Relevance: Cultivation can draw from HLA principles to design how the Laboratory integrates various models. For example, if our system needs to incorporate a fluid dynamics simulator with an AI planner, HLA’s concept of a common interface specification would allow them to communicate without being tightly coupled. We might not implement the full RTI, but we can use open-source RTIs (like Portico) or lightweight co-simulation platforms (like the FMI standard) to connect components. Notably, HLA also handles time step alignment – which is crucial if one model runs in milliseconds and another in days. Cultivation should ensure that when a hypothesis test involves multi-rate simulations, the orchestrator slows down or speeds up components so that causality is preserved (no component should use information from the “future” of another). In practice, we might incorporate an HLA-based scheduler or a custom event-loop akin to HLA’s time management.

Multi-Scale Data and Workflows: Besides simulation, scientific workflows often combine multi-scale data (e.g. genomics sequences and clinical patient data). Ensuring interoperability extends to data formats and metadata. Cultivation should adopt or support standards like HDF5 for hierarchical data or ontologies that link, say, molecular entities to organism-level phenomena. The literature (e.g. Multiscale Modeling vision by de Garrett et al., 2022) points out the need for standardized descriptions of models at different scales so they can be composed. A practical step is to use the MultiScale Modeling (MSM) ontologies or the Civil Infrastructure’s OpenMETA toolkit approach for linking models. Given time, the details might be out of scope, but the core idea is: if Cultivation can serve as a common platform where a user can plug in models of various scales and they “just work together,” that would be a powerful feature. The Topic 7 research provides the guidelines and in some cases tools to achieve this interoperability.

In summary, Topic 7 guides us to make Cultivation’s Laboratory flexible and robust in handling multi-scale phenomena. We will incorporate interoperability standards (conceptually like HLA’s modular approach) so that different simulation tools can be integrated with minimal custom glue code ￼. We’ll also integrate machine learning approaches to bridge scales (to reduce computational load), following the lead of recent physics-informed ML techniques ￼. Ultimately, this means a researcher using Cultivation could simulate a complex system end-to-end: the Think Tank might propose a mechanism linking micro and macro; the Laboratory can then actually run a coupled simulation (or surrogate-augmented simulation) to test that proposal, without the user having to manually cobble together disparate tools – the platform ensures they interoperate correctly.

8. Human-AI Collaboration Frameworks in Research and Complex Problem Solving

Hybrid Intelligence – Human + AI Teams: Akata et al. (2020) and Walther (2025) ￼. The Hybrid Intelligence Agenda (Akata, Hoos, et al.) outlined a vision for AI systems that augment rather than replace human intellect, emphasizing collaboration that is adaptive, collaborative, responsible, and explainable. Cornelia Walther’s 2025 article succinctly states “Hybrid intelligence combines the best of AI and humans, leading to more sustainable, creative, and trustworthy results.” ￼. This captures the ethos that human-AI partnerships can outperform either alone, if designed correctly. Relevance: Cultivation is itself an embodiment of hybrid intelligence – it’s meant to be a platform where human researchers and AI agents work together on complex problems. The principles from this body of work should drive our design philosophy: ensure the AI is collaborative (working with human goals, not just autonomous), adaptive (learns from and adapts to each user), responsible (e.g. aware of ethical considerations and uncertainty), and explainable. Practically, this could mean implementing features such as: the AI asks for guidance when uncertain (rather than silently making a dubious choice), the AI explains its reasoning (so humans trust and learn, aligning with “explainable” from Topic 6), and the system supports division of labor according to strengths (perhaps trivial data processing always to AI, creative hypothesis to both AI and human). The Hybrid Intelligence research also suggests measuring success not just by task outcome but by how well the human+AI team performs and learns – we should include metrics or feedback mechanisms in Cultivation to monitor the effectiveness of the collaboration (e.g. is the human accepting and acting on AI suggestions? Are AI suggestions improving with human feedback?).

Taxonomy of Human-AI Interaction: Gomez et al. (2025) ￼ conducted a systematic review of human-AI decision-making scenarios and found that most current systems use very simplistic interaction patterns – typically the AI gives a recommendation and the human either accepts or overrides, with little true back-and-forth ￼. They propose a taxonomy of richer interaction patterns and emphasize designing for clear communication, trust, and true collaboration (where the AI and human may exchange multiple times) ￼. They note that current “human-AI collaboration is not very collaborative yet,” indicating a gap. Relevance: For Cultivation, this means we should go beyond a one-shot recommendation interface. Instead of the AI just outputting a hypothesis, we can enable dialogues: the human might ask the AI follow-up questions (“How did you get that?” – tying to XAI, or “Can you refine this in light of X assumption?”) and the AI can ask the human for clarification (“Should I prioritize accuracy or novelty for the next experiment?”). Implementing such interactivity might involve a conversational interface or at least an iterative workflow where users can give feedback at each step. The taxonomy from this review can guide specific features: e.g., patterns like “AI proposes -> human adjusts -> AI refines” should be supported in the Think Tank. We might incorporate UI elements for “suggest an alternative,” “critique this result,” etc., making the user an active collaborator, not just a passive recipient of AI output. By following these patterns, we can avoid the pitfall of the human either over-relying on the AI or ignoring it; instead, we foster a symbiotic relationship where each party’s inputs are solicited and valued appropriately.

Evaluation of Human-AI Collaboration: Fragiadakis et al. (2024) ￼ ￼ provide a framework to evaluate Human-AI Collaboration (HAIC) systems. They introduce a decision-tree to select metrics for different collaboration modes – whether AI or human takes the lead or a fully symbiotic scenario ￼. The framework combines quantitative metrics (task performance, time saved) with qualitative ones (user satisfaction, trust) to capture the “dynamic and reciprocal nature” of collaboration ￼. They also discuss domain-specific considerations (e.g. in creative arts vs. healthcare, what makes a good human-AI outcome differs) ￼ ￼. Relevance: As we build Cultivation, we should integrate an evaluation component for the human-AI teaming aspects. For example, during beta tests, we can use this framework to measure how our platform affects researchers’ productivity and decision quality. Are scientists coming to better conclusions faster with Cultivation (AI-centric metric)? Are they comfortable and find it useful (human-centric metric)? Fragiadakis et al.’s mention of AI-Centric, Human-Centric, and Symbiotic modes ￼ could map to features: Cultivation might have modes where the user is steering (AI as assistant) vs. the AI is taking initiative (like an autonomous agent with oversight) vs. they iterate together. We should decide which mode Cultivation is aiming for (likely symbiotic) and ensure our evaluation matches that. Adopting their structured framework will ensure we don’t just test the AI in isolation but also how it fits into the human workflow. Over time, such evaluation can guide updates – e.g. if we find users trust the AI too little, we may need to improve explainability; if they over-trust it, we may need to encourage verification steps (like the system occasionally asks the human to double-check a critical decision, to maintain engagement and vigilance).

Collaborative Platforms and Tools: Recent years have seen practical frameworks for human-AI collaboration. Interactive Gym (McDonald, 2024) ￼ is one example where a framework allows human participants to enter an RL loop controlling agents in a simulation. It supports multiple humans and AI agents in experiments, showing that infrastructure is being built for real-time human-AI interaction in environments ￼. Another example: HHAI (Hybrid Human-AI Interaction) conferences in 2022+ share prototypes of interfaces for collaborative writing, design, etc. Relevance: For Cultivation, leveraging existing frameworks could jump-start our development of collaboration interfaces. For instance, Interactive Gym’s approach could be applied in the Laboratory: a human and an AI agent could both manipulate a simulation (human maybe guiding parameters, AI adjusting others) in a shared platform. We might not use it directly, but the idea of a web-based front-end that connects human inputs with AI agent actions in a backend environment is very applicable. Essentially, Cultivation can be seen as an “Interactive Lab” where humans and AIs jointly conduct experiments. By studying these tools, we can avoid reinventing the wheel for things like multi-user session management, real-time synchronization, etc. If the Hybrid Intelligence Center (Netherlands) releases a general framework (they were developing one codenamed “SHARPIE” as seen in [67]), we might incorporate it or at least learn from it.

In conclusion, Topic 8 literature reinforces that Cultivation’s success will depend not just on powerful AI, but on the design of the human-AI partnership. We will ensure the interface encourages iterative dialogue (taking cues from Gomez et al.’s taxonomy) ￼, that the AI agents have the ability to defer to humans or ask questions when appropriate (to avoid errors and build trust), and that humans remain in control of critical decisions (to maintain a sense of ownership and responsibility). We’ll also incorporate collaborative features like shared workspaces where multiple human team members and AI can all see the project state and contribute (since research is often a team sport). By grounding our design in the emerging frameworks of hybrid intelligence ￼ and rigorous evaluation criteria ￼ ￼, we aim to create a system where the human-AI team is greater than the sum of its parts – truly accelerating discovery in a responsible, user-friendly manner.

⸻

Sources:
	1.	Alkan, A. K., et al. (2025). A Survey on Hypothesis Generation for Scientific Discovery in the Era of LLMs. arXiv:2504.05496 ￼
	2.	Kulkarni, A., et al. (2025). Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions. arXiv:2505.04651 ￼ ￼
	3.	Ghafarollahi, A. & Buehler, M. J. (2024). SciAgents: Automating Scientific Discovery through Multi-agent Intelligent Graph Reasoning. arXiv:2409.05556 ￼
	4.	Zhou, Y., et al. (2024). Hypothesis Generation with Large Language Models. arXiv:2404.04326 ￼ ￼
	5.	Aharoni, S., et al. (2025). Sparks of Science: Hypothesis Generation Using Structured Paper Data (HypoGen) – Code: GitHub UniverseTBD/hypogen ￼ ￼
	6.	Lu, S., et al. (2024). The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery. arXiv:2408.06292 (ICLR AI4Science) ￼
	7.	Liu, L., et al. (2024). Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective. arXiv:2412.10390 ￼
	8.	Zhu, Y., et al. (2023). LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities. arXiv:2305.13168 ￼ ￼
	9.	Li, J., et al. (2024). Hierarchical Tree-Structured Knowledge Graph for Academic Insight Survey. arXiv:2402.04854 ￼ ￼
	10.	Chen, Z., et al. (2025). Analogical Learning for Cross-Scenario Generalization (Mateformer). arXiv:2504.08811 ￼ ￼
	11.	Xie, F., et al. (2024). Large Language Models as Analogical Reasoners (Analogical Prompting). arXiv:2310.01714 (ICLR 2024) ￼ ￼
	12.	Musker, S., et al. (2024). LLMs as Models for Analogical Reasoning. arXiv:2406.13803 ￼ ￼
	13.	Saha, A., et al. (2024). Analogical Reasoning on Narratives (ARN) Benchmark. arXiv:2310.00996 ￼ ￼
	14.	Bellomo, S., et al. (2025). SciRep: A Framework for Reproducibility of Computational Experiments. arXiv:2503.07080 ￼ ￼
	15.	Barbosa, S., et al. (2025). A Dataset for Computational Reproducibility. arXiv:2504.08684 ￼ ￼
	16.	Beber, M. (2025). In Pursuit of Total Reproducibility: Marrying Event Sourcing with Computational Systems Biology. arXiv:2504.11635 ￼
	17.	Marzolla, M. & D’Angelo, G. (2020). Parallel Data Distribution Management on Shared-Memory Multiprocessors (HLA DDM). ACM TOMACS 30(1) ￼
	18.	Xiao, Q. & Wang, J. (2024). DRL-SRS: A Deep Reinforcement Learning Approach for Optimizing Spaced Repetition Scheduling. Appl. Sci. 14(13):5591 ￼ ￼
	19.	Tabibian, B., et al. (2019). Enhancing Human Learning via Spaced Repetition Optimization. PNAS, 116(10):3988–3993 ￼
	20.	Shen, S., et al. (2024). A Survey of Knowledge Tracing: Models, Variants, and Applications. IEEE Trans. Learning Technologies 17:1898–1919 ￼ ￼
	21.	Li, Z., et al. (2022). From Kepler to Newton: Explainable AI for Science. arXiv:2111.12210 (ICML AI4Science 2022) ￼ ￼
	22.	Mengaldo, G. (2025). Explainable AI and the Scientific Method: Interpretability-Guided Knowledge Discovery. arXiv:2406.10557 ￼ ￼
	23.	Iser, M. (2024). Automated Explanation Selection for Scientific Discovery. arXiv:2407.17454 ￼
	24.	Gomez, C., et al. (2025). Human-AI collaboration is not very collaborative yet: Taxonomy of Interaction Patterns. Front. Comput. Sci. 6:1521066 ￼
	25.	Fragiadakis, G., et al. (2024). Evaluating Human-AI Collaboration: A Review and Methodological Framework. arXiv:2407.19098 ￼ ￼
	26.	Cornelia C. Walther (2025). Why Hybrid Intelligence Is the Future of Human-AI Collaboration. Knowledge@Wharton ￼
	27.	McDonald, C. (2024). Interactive Gym: A Framework for Human-AI Interaction in RL. (GitHub) ￼